{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MASTER ETL SCRIPT --- START ---\n",
      ">>> MASTER ETL SCRIPT: Starting Banking ETL Process <<<\n",
      "\n",
      "--- Starting Step 1: Ingestion, Decoding & Metadata ---\n",
      "  [Meta] Setting up connection...\n",
      "  [Meta] Connection established.\n",
      "  [Meta] Table check/creation completed.\n",
      "  [Config] Loading column map from: https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/column_table_map.json\n",
      "  [Config] Column mapping loaded successfully.\n",
      "\n",
      "  [Main] Starting file ingestion and decoding loop (using in-code config)...\n",
      "\n",
      "    >>> Processing: t01.csv (Logical: users, TableID: 01)\n",
      "      Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t01.csv...\n",
      "      Download complete.\n",
      "      Reading CSV...\n",
      "      Read complete: 600 rows.\n",
      "      Saving raw data to raw_data\\t01.csv...\n",
      "      Raw data saved.\n",
      "      Decoding columns...\n",
      "        Columns decoded. Example: ['id', 'name', 'phone_number', 'email', 'created_at']...\n",
      "      Decoding complete.\n",
      "      Storing 'users' in memory.\n",
      "      Saving decoded data to decoded_data\\decoded_t01.csv...\n",
      "      Decoded data saved.\n",
      "      >>> Successfully processed t01.csv.\n",
      "      Logging metadata for t01.csv...\n",
      "      Metadata logged.\n",
      "\n",
      "    >>> Processing: t02.csv (Logical: cards, TableID: 02)\n",
      "      Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t02.csv...\n",
      "      Download complete.\n",
      "      Reading CSV...\n",
      "      Read complete: 1200 rows.\n",
      "      Saving raw data to raw_data\\t02.csv...\n",
      "      Raw data saved.\n",
      "      Decoding columns...\n",
      "        Columns decoded. Example: ['id', 'user_id', 'card_number', 'balance', 'created_at']...\n",
      "      Decoding complete.\n",
      "      Storing 'cards' in memory.\n",
      "      Saving decoded data to decoded_data\\decoded_t02.csv...\n",
      "      Decoded data saved.\n",
      "      >>> Successfully processed t02.csv.\n",
      "      Logging metadata for t02.csv...\n",
      "      Metadata logged.\n",
      "\n",
      "    >>> Processing: t03.csv (Logical: transactions, TableID: 03)\n",
      "      Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t03.csv...\n",
      "      Download complete.\n",
      "      Reading CSV...\n",
      "      Read complete: 3000 rows.\n",
      "      Saving raw data to raw_data\\t03.csv...\n",
      "      Raw data saved.\n",
      "      Decoding columns...\n",
      "        Columns decoded. Example: ['id', 'from_card_id', 'to_card_id', 'amount', 'status']...\n",
      "      Decoding complete.\n",
      "      Storing 'transactions' in memory.\n",
      "      Saving decoded data to decoded_data\\decoded_t03.csv...\n",
      "      Decoded data saved.\n",
      "      >>> Successfully processed t03.csv.\n",
      "      Logging metadata for t03.csv...\n",
      "      Metadata logged.\n",
      "\n",
      "    >>> Processing: t04.csv (Logical: logs, TableID: 04)\n",
      "      Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t04.csv...\n",
      "      Download complete.\n",
      "      Reading CSV...\n",
      "      Read complete: 2014 rows.\n",
      "      Saving raw data to raw_data\\t04.csv...\n",
      "      Raw data saved.\n",
      "      Decoding columns...\n",
      "        Columns decoded. Example: ['id', 'transaction_id', 'message', 'created_at']...\n",
      "      Decoding complete.\n",
      "      Storing 'logs' in memory.\n",
      "      Saving decoded data to decoded_data\\decoded_t04.csv...\n",
      "      Decoded data saved.\n",
      "      >>> Successfully processed t04.csv.\n",
      "      Logging metadata for t04.csv...\n",
      "      Metadata logged.\n",
      "\n",
      "    >>> Processing: t05.csv (Logical: reports, TableID: 05)\n",
      "      Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t05.csv...\n",
      "      Download complete.\n",
      "      Reading CSV...\n",
      "      Read complete: 92 rows.\n",
      "      Saving raw data to raw_data\\t05.csv...\n",
      "      Raw data saved.\n",
      "      Decoding columns...\n",
      "        Columns decoded. Example: ['id', 'report_type', 'created_at', 'total_transactions', 'flagged_transactions']...\n",
      "      Decoding complete.\n",
      "      Storing 'reports' in memory.\n",
      "      Saving decoded data to decoded_data\\decoded_t05.csv...\n",
      "      Decoded data saved.\n",
      "      >>> Successfully processed t05.csv.\n",
      "      Logging metadata for t05.csv...\n",
      "      Metadata logged.\n",
      "\n",
      "    >>> Processing: t07.csv (Logical: scheduled_payments, TableID: 07)\n",
      "      Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t07.csv...\n",
      "      Download complete.\n",
      "      Reading CSV...\n",
      "      Read complete: 800 rows.\n",
      "      Saving raw data to raw_data\\t07.csv...\n",
      "      Raw data saved.\n",
      "      Decoding columns...\n",
      "        Columns decoded. Example: ['id', 'user_id', 'card_id', 'amount', 'payment_date']...\n",
      "      Decoding complete.\n",
      "      Storing 'scheduled_payments' in memory.\n",
      "      Saving decoded data to decoded_data\\decoded_t07.csv...\n",
      "      Decoded data saved.\n",
      "      >>> Successfully processed t07.csv.\n",
      "      Logging metadata for t07.csv...\n",
      "      Metadata logged.\n",
      "\n",
      "  [Main] Ingestion and decoding loop finished.\n",
      "  [Meta] Connection closed.\n",
      "--- Step 1 Finished. Overall Success: True ---\n",
      "\n",
      "--- Starting Step 2: Data Cleaning ---\n",
      "    PLACEHOLDER: Data cleaning functions need to be fully implemented here from your Script 2.\n",
      "    Cleaning (placeholder) for: users\n",
      "      Saved cleaned (placeholder) data for users to cleaned_data\\cleaned_users.csv\n",
      "    Cleaning (placeholder) for: cards\n",
      "      Saved cleaned (placeholder) data for cards to cleaned_data\\cleaned_cards.csv\n",
      "    Cleaning (placeholder) for: transactions\n",
      "      Saved cleaned (placeholder) data for transactions to cleaned_data\\cleaned_transactions.csv\n",
      "    Cleaning (placeholder) for: logs\n",
      "      Saved cleaned (placeholder) data for logs to cleaned_data\\cleaned_logs.csv\n",
      "    Cleaning (placeholder) for: reports\n",
      "      Saved cleaned (placeholder) data for reports to cleaned_data\\cleaned_reports.csv\n",
      "    Cleaning (placeholder) for: scheduled_payments\n",
      "      Saved cleaned (placeholder) data for scheduled_payments to cleaned_data\\cleaned_scheduled_payments.csv\n",
      "--- Step 2 Finished. Success: True ---\n",
      "\n",
      "--- Starting Step 3: Load to SQL Server ---\n",
      "    PLACEHOLDER: SQL Loading logic needs to be implemented here from your Script 3.\n",
      "--- Step 3 Finished. Success: True ---\n",
      "\n",
      "--- Starting Step 5: Create SQL Views ---\n",
      "    PLACEHOLDER: SQL View creation logic needs to be implemented here.\n",
      "--- Step 5 Finished. Success: True ---\n",
      "\n",
      ">>> Banking ETL Process Finished in 0:00:08.721892 <<<\n",
      "All ETL steps reported success!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# MASTER ETL SCRIPT (Python Automation) - Step 1 Yangilangan\n",
    "# ==============================================================================\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import pyodbc\n",
    "import datetime\n",
    "import numpy as np # Keyingi bosqichlar uchun kerak bo'lishi mumkin\n",
    "import re          # Keyingi bosqichlar uchun kerak bo'lishi mumkin\n",
    "\n",
    "print(\"--- MASTER ETL SCRIPT --- START ---\")\n",
    "start_time_total = datetime.datetime.now()\n",
    "\n",
    "# --- Global Konfiguratsiyalar ---\n",
    "# GitHub\n",
    "REPO_RAW_BASE = \"https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final\"\n",
    "COLUMN_MAP_URL = \"https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/column_table_map.json\"\n",
    "# Fayl Mapping Konfiguratsiyasi KOD ICHIDA BO'LADI (Skript 1 logikasi bo'yicha)\n",
    "\n",
    "# Papkalar\n",
    "RAW_DIR = \"raw_data\"; DECODED_DIR = \"decoded_data\"; CLEANED_DIR = \"cleaned_data\"\n",
    "os.makedirs(RAW_DIR, exist_ok=True); os.makedirs(DECODED_DIR, exist_ok=True); os.makedirs(CLEANED_DIR, exist_ok=True)\n",
    "\n",
    "# SQL Server\n",
    "DB_SERVER = 'WIN-A5I5TM5OKQQ\\\\SQLEXPRESS'; DB_DATABASE = 'BankingDB'; USE_TRUSTED_CONNECTION = True; driver = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "# ... (Boshqa global sozlamalar kerak bo'lsa) ...\n",
    "\n",
    "\n",
    "# --- Yordamchi Funksiyalar (SQL Type, Value Conversion - keyingi bosqichlar uchun) ---\n",
    "def get_sql_type(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype): return 'BIGINT'\n",
    "    elif pd.api.types.is_float_dtype(dtype): return 'FLOAT'\n",
    "    elif pd.api.types.is_datetime64_any_dtype(dtype): return 'DATETIME2'\n",
    "    elif pd.api.types.is_bool_dtype(dtype): return 'BIT'\n",
    "    else: return 'NVARCHAR(MAX)'\n",
    "\n",
    "def convert_value(item): # NumPy/Pandas ni Python ga o'girish\n",
    "    if pd.isna(item): return None\n",
    "    if isinstance(item, (np.integer)): return int(item)\n",
    "    if isinstance(item, (np.floating)): return float(item)\n",
    "    if isinstance(item, (np.bool_)): return bool(item)\n",
    "    return item\n",
    "\n",
    "# ==============================================================================\n",
    "# BOSQICH 1: Ingestion, Decoding, Metadata (Siz yuborgan skript asosida)\n",
    "# ==============================================================================\n",
    "def run_step1_ingest_decode_metadata():\n",
    "    print(\"\\n--- Starting Step 1: Ingestion, Decoding & Metadata ---\")\n",
    "    # >>> Fayl Mapping Konfiguratsiyasi KOD ICHIDA <<<\n",
    "    file_mapping_config = {\n",
    "        't01.csv': {'logical_name': 'users', 'table_id': '01'},\n",
    "        't02.csv': {'logical_name': 'cards', 'table_id': '02'},\n",
    "        't03.csv': {'logical_name': 'transactions', 'table_id': '03'},\n",
    "        't04.csv': {'logical_name': 'logs', 'table_id': '04'},\n",
    "        't05.csv': {'logical_name': 'reports', 'table_id': '05'},\n",
    "        't07.csv': {'logical_name': 'scheduled_payments', 'table_id': '07'},\n",
    "    }\n",
    "    dataframes_ingested = {} # Bu funksiya qaytaradigan natija\n",
    "    column_map_data_from_json = None\n",
    "    cnxn_meta = None; cursor_meta = None\n",
    "    step1_success = True\n",
    "\n",
    "    try:\n",
    "        # Metadata ulanishi\n",
    "        print(\"  [Meta] Setting up connection...\")\n",
    "        if USE_TRUSTED_CONNECTION: conn_str_meta = (f\"DRIVER={driver};SERVER={DB_SERVER};DATABASE={DB_DATABASE};Trusted_Connection=yes;\")\n",
    "        else: pass # SQL Auth\n",
    "        cnxn_meta = pyodbc.connect(conn_str_meta, autocommit=False)\n",
    "        cursor_meta = cnxn_meta.cursor(); print(\"  [Meta] Connection established.\")\n",
    "        create_meta_sql = \"IF OBJECT_ID('dbo.retrieveinfo', 'U') IS NULL BEGIN CREATE TABLE dbo.retrieveinfo ( retrieve_id INT IDENTITY(1,1) PRIMARY KEY, source_file NVARCHAR(100) NOT NULL, retrieved_at DATETIME2 NOT NULL, total_rows INT NULL, processed_rows INT NULL, errors INT DEFAULT 0, notes NVARCHAR(MAX) NULL ); PRINT '  [Meta] Table retrieveinfo created.'; END ELSE BEGIN PRINT '  [Meta] Table retrieveinfo already exists.'; END\"\n",
    "        cursor_meta.execute(create_meta_sql); cnxn_meta.commit(); print(\"  [Meta] Table check/creation completed.\")\n",
    "\n",
    "        # Ustun Nomlari Xaritasini Yuklash\n",
    "        print(f\"  [Config] Loading column map from: {COLUMN_MAP_URL}\")\n",
    "        resp_col_map = requests.get(COLUMN_MAP_URL)\n",
    "        resp_col_map.raise_for_status()\n",
    "        column_map_data_from_json = resp_col_map.json()\n",
    "        if not isinstance(column_map_data_from_json, dict) or not column_map_data_from_json:\n",
    "            raise ValueError(\"Column map JSON is invalid or empty.\")\n",
    "        print(\"  [Config] Column mapping loaded successfully.\")\n",
    "\n",
    "        # Fayllarni Yuklash, Dekodlash, Metadata Yozish\n",
    "        print(\"\\n  [Main] Starting file ingestion and decoding loop (using in-code config)...\")\n",
    "        for filename, info in file_mapping_config.items():\n",
    "            logical_name = info.get('logical_name')\n",
    "            table_id = info.get('table_id')\n",
    "            file_url = f\"{REPO_RAW_BASE}/{filename}\"\n",
    "            retrieval_start_time = datetime.datetime.now()\n",
    "            total_rows_read, processed_rows_count = None, None\n",
    "            error_flag, error_notes = 0, None\n",
    "            df = pd.DataFrame()\n",
    "\n",
    "            print(f\"\\n    >>> Processing: {filename} (Logical: {logical_name}, TableID: {table_id})\")\n",
    "            try:\n",
    "                print(f\"      Downloading from {file_url}...\");\n",
    "                resp_csv = requests.get(file_url); resp_csv.raise_for_status(); print(\"      Download complete.\")\n",
    "                print(\"      Reading CSV...\");\n",
    "                try: df = pd.read_csv(io.StringIO(resp_csv.text))\n",
    "                except UnicodeDecodeError: df = pd.read_csv(io.StringIO(resp_csv.content.decode('latin1')))\n",
    "                total_rows_read = len(df); print(f\"      Read complete: {total_rows_read} rows.\")\n",
    "                raw_path = os.path.join(RAW_DIR, filename); print(f\"      Saving raw data to {raw_path}...\");\n",
    "                with open(raw_path, 'wb') as f: f.write(resp_csv.content); print(\"      Raw data saved.\")\n",
    "\n",
    "                print(\"      Decoding columns...\");\n",
    "                table_specific_map = column_map_data_from_json.get(table_id, {}).get(\"columns\", {})\n",
    "                if not table_specific_map:\n",
    "                    decoded_columns = df.columns.tolist(); print(f\"        Warning: No mapping for table_id '{table_id}'. Keeping original names.\")\n",
    "                else:\n",
    "                    decoded_columns = [table_specific_map.get(col.split(\"-\")[-1], col) for col in df.columns]; df.columns = decoded_columns;\n",
    "                    print(f\"        Columns decoded. Example: {decoded_columns[:5]}...\")\n",
    "                processed_rows_count = len(df); print(\"      Decoding complete.\")\n",
    "\n",
    "                print(f\"      Storing '{logical_name}' in memory.\"); dataframes_ingested[logical_name] = df\n",
    "                decoded_path = os.path.join(DECODED_DIR, f\"decoded_{filename}\"); print(f\"      Saving decoded data to {decoded_path}...\");\n",
    "                df.to_csv(decoded_path, index=False, encoding='utf-8'); print(\"      Decoded data saved.\")\n",
    "                error_flag = 0; error_notes = \"Success\"; print(f\"      >>> Successfully processed {filename}.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                error_flag=1; error_notes=f\"Error with {filename}: {e}\"; print(f\"      ERROR: {error_notes}\"); dataframes_ingested[logical_name]=pd.DataFrame()\n",
    "                step1_success = False # Agar birorta fayl xato bersa, umumiy statusni false qilamiz\n",
    "\n",
    "            # Metadata yozish\n",
    "            if cursor_meta and cnxn_meta:\n",
    "                print(f\"      Logging metadata for {filename}...\");\n",
    "                try:\n",
    "                    sql_meta=\"INSERT INTO retrieveinfo (source_file, retrieved_at, total_rows, processed_rows, errors, notes) VALUES (?, ?, ?, ?, ?, ?)\"\n",
    "                    meta_tuple=(filename, retrieval_start_time, total_rows_read, processed_rows_count, error_flag, str(error_notes)[:4000])\n",
    "                    cursor_meta.execute(sql_meta, meta_tuple); cnxn_meta.commit(); print(\"      Metadata logged.\")\n",
    "                except Exception as meta_ex:\n",
    "                    print(f\"      ERROR logging metadata for {filename}: {meta_ex}\"); cnxn_meta.rollback()\n",
    "                    step1_success = False # Metadata yozishda xato bo'lsa ham\n",
    "            else:\n",
    "                print(\"      Skipping metadata logging (DB connection not available for this file).\")\n",
    "            time.sleep(0.1)\n",
    "        print(\"\\n  [Main] Ingestion and decoding loop finished.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR in Step 1 main process: {e}\")\n",
    "        step1_success = False\n",
    "    finally:\n",
    "        if cnxn_meta:\n",
    "            if cursor_meta: cursor_meta.close()\n",
    "            cnxn_meta.close(); print(\"  [Meta] Connection closed.\")\n",
    "        print(f\"--- Step 1 Finished. Overall Success: {step1_success} ---\")\n",
    "    return dataframes_ingested if step1_success else None\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# BOSQICH 2: Data Cleaning (Placeholder - Buni ham to'ldirishingiz kerak)\n",
    "# ==============================================================================\n",
    "def run_step2_cleaning(dataframes_to_clean):\n",
    "    print(\"\\n--- Starting Step 2: Data Cleaning ---\")\n",
    "    if dataframes_to_clean is None:\n",
    "        print(\"  ERROR: No dataframes from Step 1. Skipping cleaning.\")\n",
    "        return None\n",
    "    cleaned_dataframes_output = {}\n",
    "    step2_success = True\n",
    "    # --- Tozalash Sozlamalari ---\n",
    "    TRANSACTION_AMOUNT_THRESHOLD_CLEAN = 15000 # Nomenklaturani o'zgartirdim\n",
    "    EMAIL_REGEX_CLEAN = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
    "    PHONE_REGEX_CLEAN = r\"^(?=.*\\d{6,})[\\d\\s\\+\\-\\(\\)]+$\"\n",
    "\n",
    "    # --- Yordamchi Funksiyalar (Tozalash uchun) ---\n",
    "    def safe_to_datetime(series, **kwargs): return pd.to_datetime(series, errors='coerce', **kwargs)\n",
    "    def safe_to_numeric(series, **kwargs): return pd.to_numeric(series, errors='coerce', **kwargs)\n",
    "    def safe_to_boolean(series):\n",
    "        if series.dtype == 'bool': return series\n",
    "        map_dict = {'true':True,'false':False,'1':True,'0':False,1:True,0:False,'yes':True,'no':False}\n",
    "        return series.apply(lambda x: str(x).lower() if pd.notna(x) else x).map(map_dict).fillna(False).astype(bool)\n",
    "    def safe_to_int64(series):\n",
    "        numeric_series = safe_to_numeric(series)\n",
    "        try: return numeric_series.astype('Int64')\n",
    "        except: return numeric_series\n",
    "\n",
    "    def clean_dataframe_generic(df_orig, logical_name_clean): # Umumiy tozalash\n",
    "        print(f\"    Cleaning generic: {logical_name_clean}, Original shape: {df_orig.shape if df_orig is not None else 'None'}\")\n",
    "        if df_orig is None or df_orig.empty: return pd.DataFrame()\n",
    "        df = df_orig.copy()\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "            if col in df.columns and pd.api.types.is_string_dtype(df[col]):\n",
    "                try: df[col] = df[col].str.strip()\n",
    "                except: pass\n",
    "        return df\n",
    "\n",
    "    # ... (Sizning clean_users, clean_cards, ... funksiyalaringizni SHU YERGA KO'CHIRING) ...\n",
    "    # Funksiyalar ichidagi ustun nomlari Skript 1 dan keladigan 'id', 'name' kabi\n",
    "    # dekodlangan nomlarga mos bo'lishi kerak.\n",
    "\n",
    "    print(\"    PLACEHOLDER: Data cleaning functions need to be fully implemented here from your Script 2.\")\n",
    "    # Misol uchun, har bir DataFrame ni shunchaki nusxalab o'tkazamiz:\n",
    "    for name, df_orig_clean in dataframes_to_clean.items():\n",
    "        print(f\"    Cleaning (placeholder) for: {name}\")\n",
    "        # cleaned_df = your_specific_cleaning_function(df_orig_clean, other_data_if_needed)\n",
    "        cleaned_df = df_orig_clean.copy() # Hozircha\n",
    "        cleaned_dataframes_output[name] = cleaned_df\n",
    "        # Tozalangan faylni saqlash\n",
    "        os.makedirs(CLEANED_DIR, exist_ok=True)\n",
    "        save_path = os.path.join(CLEANED_DIR, f\"cleaned_{name}.csv\")\n",
    "        cleaned_df.to_csv(save_path, index=False, encoding='utf-8')\n",
    "        print(f\"      Saved cleaned (placeholder) data for {name} to {save_path}\")\n",
    "\n",
    "\n",
    "    print(f\"--- Step 2 Finished. Success: {step2_success} ---\") # Xatolik boshqaruvini qo'shish kerak\n",
    "    return cleaned_dataframes_output # Yoki xato bo'lsa None\n",
    "\n",
    "# ==============================================================================\n",
    "# BOSQICH 3: Load to SQL (Placeholder)\n",
    "# ==============================================================================\n",
    "def run_step3_load_to_sql(data_to_load):\n",
    "    print(\"\\n--- Starting Step 3: Load to SQL Server ---\")\n",
    "    if data_to_load is None:\n",
    "        print(\"  ERROR: No cleaned data. Skipping SQL load.\")\n",
    "        return False\n",
    "    step3_success = True\n",
    "    # ... (Sizning SQLga yuklash, PK/FK qo'shish uchun Skript 3 logikangizni SHU YERGA KO'CHIRING) ...\n",
    "    # NumPy konvertatsiyasi bilan oxirgi ishlagan versiyani oling.\n",
    "    # Har bir jadvalni alohida yuklash yondashuvini ishlatishingiz mumkin.\n",
    "    print(\"    PLACEHOLDER: SQL Loading logic needs to be implemented here from your Script 3.\")\n",
    "    print(f\"--- Step 3 Finished. Success: {step3_success} ---\")\n",
    "    return step3_success\n",
    "\n",
    "# ==============================================================================\n",
    "# BOSQICH 5: SQL Transformations (Create Views - Placeholder)\n",
    "# ==============================================================================\n",
    "def run_step5_sql_views():\n",
    "    print(\"\\n--- Starting Step 5: Create SQL Views ---\")\n",
    "    step5_success = True\n",
    "    # ... (Sizning VIEW yaratish SQL skriptingizni pyodbc orqali bajarish logikasi SHU YERGA) ...\n",
    "    print(\"    PLACEHOLDER: SQL View creation logic needs to be implemented here.\")\n",
    "    print(f\"--- Step 5 Finished. Success: {step5_success} ---\")\n",
    "    return step5_success\n",
    "\n",
    "# ==============================================================================\n",
    "# ASOSIY ISH OQIMI\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\">>> MASTER ETL SCRIPT: Starting Banking ETL Process <<<\")\n",
    "\n",
    "    # Bosqich 1\n",
    "    ingested_data = run_step1_ingest_decode_metadata()\n",
    "\n",
    "    # Bosqich 2\n",
    "    cleaned_data = None\n",
    "    if ingested_data:\n",
    "        cleaned_data = run_step2_cleaning(ingested_data)\n",
    "    else:\n",
    "        print(\"Skipping Step 2 due to errors or no data from Step 1.\")\n",
    "\n",
    "    # Bosqich 3\n",
    "    load_sql_success = False\n",
    "    if cleaned_data:\n",
    "        load_sql_success = run_step3_load_to_sql(cleaned_data)\n",
    "    else:\n",
    "        print(\"Skipping Step 3 due to errors or no data from Step 2.\")\n",
    "\n",
    "    # Bosqich 5\n",
    "    views_success = False\n",
    "    if load_sql_success:\n",
    "        views_success = run_step5_sql_views()\n",
    "    else:\n",
    "        print(\"Skipping Step 5 due to errors in Step 3.\")\n",
    "\n",
    "    end_time_total = datetime.datetime.now()\n",
    "    print(f\"\\n>>> Banking ETL Process Finished in {end_time_total - start_time_total} <<<\")\n",
    "\n",
    "    if ingested_data and cleaned_data and load_sql_success and views_success:\n",
    "        print(\"All ETL steps reported success!\")\n",
    "    else:\n",
    "        print(\"ETL Process completed with one or more reported errors. Please review logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
