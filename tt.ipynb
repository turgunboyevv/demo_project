{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MASTER ETL SCRIPT --- START ---\n",
      ">>> MASTER ETL SCRIPT: Starting Banking ETL Process <<<\n",
      "\n",
      "--- Starting Step 1: Ingestion, Decoding & Metadata ---\n",
      "  [Meta] Setting up connection...\n",
      "  [Meta] Connection established.\n",
      "  [Meta] Table check/creation completed.\n",
      "  [Config] Loading column map from: https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/column_table_map.json\n",
      "  [Config] Column mapping loaded successfully.\n",
      "\n",
      "  [Main] Starting file ingestion and decoding loop (using in-code config)...\n",
      "\n",
      "    >>> Processing: t01.csv (Logical: users, TableID: 01)\n",
      "      Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t01.csv...\n",
      "      Download complete.\n",
      "      Reading CSV...\n",
      "      Read complete: 600 rows.\n",
      "      Saving raw data to raw_data\\t01.csv...\n",
      "      Raw data saved.\n",
      "      Decoding columns...\n",
      "        Columns decoded. Example: ['id', 'name', 'phone_number', 'email', 'created_at']...\n",
      "      Decoding complete.\n",
      "      Storing 'users' in memory.\n",
      "      Saving decoded data to decoded_data\\decoded_t01.csv...\n",
      "      Decoded data saved.\n",
      "      >>> Successfully processed t01.csv.\n",
      "      Logging metadata for t01.csv...\n",
      "      Metadata logged.\n",
      "\n",
      "    >>> Processing: t02.csv (Logical: cards, TableID: 02)\n",
      "      Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t02.csv...\n",
      "      Download complete.\n",
      "      Reading CSV...\n",
      "      Read complete: 1200 rows.\n",
      "      Saving raw data to raw_data\\t02.csv...\n",
      "      Raw data saved.\n",
      "      Decoding columns...\n",
      "        Columns decoded. Example: ['id', 'user_id', 'card_number', 'balance', 'created_at']...\n",
      "      Decoding complete.\n",
      "      Storing 'cards' in memory.\n",
      "      Saving decoded data to decoded_data\\decoded_t02.csv...\n",
      "      Decoded data saved.\n",
      "      >>> Successfully processed t02.csv.\n",
      "      Logging metadata for t02.csv...\n",
      "      Metadata logged.\n",
      "\n",
      "    >>> Processing: t03.csv (Logical: transactions, TableID: 03)\n",
      "      Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t03.csv...\n",
      "      Download complete.\n",
      "      Reading CSV...\n",
      "      Read complete: 3000 rows.\n",
      "      Saving raw data to raw_data\\t03.csv...\n",
      "      Raw data saved.\n",
      "      Decoding columns...\n",
      "        Columns decoded. Example: ['id', 'from_card_id', 'to_card_id', 'amount', 'status']...\n",
      "      Decoding complete.\n",
      "      Storing 'transactions' in memory.\n",
      "      Saving decoded data to decoded_data\\decoded_t03.csv...\n",
      "      Decoded data saved.\n",
      "      >>> Successfully processed t03.csv.\n",
      "      Logging metadata for t03.csv...\n",
      "      Metadata logged.\n",
      "\n",
      "    >>> Processing: t04.csv (Logical: logs, TableID: 04)\n",
      "      Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t04.csv...\n",
      "      Download complete.\n",
      "      Reading CSV...\n",
      "      Read complete: 2014 rows.\n",
      "      Saving raw data to raw_data\\t04.csv...\n",
      "      Raw data saved.\n",
      "      Decoding columns...\n",
      "        Columns decoded. Example: ['id', 'transaction_id', 'message', 'created_at']...\n",
      "      Decoding complete.\n",
      "      Storing 'logs' in memory.\n",
      "      Saving decoded data to decoded_data\\decoded_t04.csv...\n",
      "      Decoded data saved.\n",
      "      >>> Successfully processed t04.csv.\n",
      "      Logging metadata for t04.csv...\n",
      "      Metadata logged.\n",
      "\n",
      "    >>> Processing: t05.csv (Logical: reports, TableID: 05)\n",
      "      Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t05.csv...\n",
      "      Download complete.\n",
      "      Reading CSV...\n",
      "      Read complete: 92 rows.\n",
      "      Saving raw data to raw_data\\t05.csv...\n",
      "      Raw data saved.\n",
      "      Decoding columns...\n",
      "        Columns decoded. Example: ['id', 'report_type', 'created_at', 'total_transactions', 'flagged_transactions']...\n",
      "      Decoding complete.\n",
      "      Storing 'reports' in memory.\n",
      "      Saving decoded data to decoded_data\\decoded_t05.csv...\n",
      "      Decoded data saved.\n",
      "      >>> Successfully processed t05.csv.\n",
      "      Logging metadata for t05.csv...\n",
      "      Metadata logged.\n",
      "\n",
      "    >>> Processing: t07.csv (Logical: scheduled_payments, TableID: 07)\n",
      "      Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t07.csv...\n",
      "      Download complete.\n",
      "      Reading CSV...\n",
      "      Read complete: 800 rows.\n",
      "      Saving raw data to raw_data\\t07.csv...\n",
      "      Raw data saved.\n",
      "      Decoding columns...\n",
      "        Columns decoded. Example: ['id', 'user_id', 'card_id', 'amount', 'payment_date']...\n",
      "      Decoding complete.\n",
      "      Storing 'scheduled_payments' in memory.\n",
      "      Saving decoded data to decoded_data\\decoded_t07.csv...\n",
      "      Decoded data saved.\n",
      "      >>> Successfully processed t07.csv.\n",
      "      Logging metadata for t07.csv...\n",
      "      Metadata logged.\n",
      "\n",
      "  [Main] Ingestion and decoding loop finished.\n",
      "  [Meta] Connection closed.\n",
      "--- Step 1 Finished. Overall Success: True ---\n",
      "\n",
      "--- Starting Step 2: Data Cleaning ---\n",
      "    PLACEHOLDER: Data cleaning functions need to be fully implemented here from your Script 2.\n",
      "    Cleaning (placeholder) for: users\n",
      "      Saved cleaned (placeholder) data for users to cleaned_data\\cleaned_users.csv\n",
      "    Cleaning (placeholder) for: cards\n",
      "      Saved cleaned (placeholder) data for cards to cleaned_data\\cleaned_cards.csv\n",
      "    Cleaning (placeholder) for: transactions\n",
      "      Saved cleaned (placeholder) data for transactions to cleaned_data\\cleaned_transactions.csv\n",
      "    Cleaning (placeholder) for: logs\n",
      "      Saved cleaned (placeholder) data for logs to cleaned_data\\cleaned_logs.csv\n",
      "    Cleaning (placeholder) for: reports\n",
      "      Saved cleaned (placeholder) data for reports to cleaned_data\\cleaned_reports.csv\n",
      "    Cleaning (placeholder) for: scheduled_payments\n",
      "      Saved cleaned (placeholder) data for scheduled_payments to cleaned_data\\cleaned_scheduled_payments.csv\n",
      "--- Step 2 Finished. Success: True ---\n",
      "\n",
      "--- Starting Step 3: Load to SQL Server ---\n",
      "    PLACEHOLDER: SQL Loading logic needs to be implemented here from your Script 3.\n",
      "--- Step 3 Finished. Success: True ---\n",
      "\n",
      "--- Starting Step 5: Create SQL Views ---\n",
      "    PLACEHOLDER: SQL View creation logic needs to be implemented here.\n",
      "--- Step 5 Finished. Success: True ---\n",
      "\n",
      ">>> Banking ETL Process Finished in 0:00:08.721892 <<<\n",
      "All ETL steps reported success!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "import pyodbc\n",
    "\n",
    "# Names and URLs\n",
    "SQL_SERVER = \"SANJARBEK\\\\SQLEXPRESS\"\n",
    "CLEANED_DB  = \"BankingETL\"\n",
    "FILES_URL   = \"https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final\"\n",
    "MAP_URL     = \"https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/column_table_map.json\"\n",
    "\n",
    "# Load column map JSON\n",
    "resp = requests.get(MAP_URL); resp.raise_for_status()\n",
    "column_map = resp.json()\n",
    "\n",
    "# Dynamically build TABLE_IDS and CSV_FILES\n",
    "TABLE_IDS = {\n",
    "    tid: meta[\"table\"]\n",
    "    for tid, meta in column_map.items()\n",
    "    if \"columns\" in meta and tid.isdigit()\n",
    "}\n",
    "CSV_FILES = {\n",
    "    tid: meta[\"file\"]\n",
    "    for tid, meta in column_map.items()\n",
    "    if \"columns\" in meta and tid.isdigit()\n",
    "}\n",
    "\n",
    "# Output directory\n",
    "CLEANED_DIR = \"cleaned_data_csv\"\n",
    "os.makedirs(CLEANED_DIR, exist_ok=True)\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "def get_engine(db):\n",
    "    conn_str = urllib.parse.quote_plus(\n",
    "        f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "        f\"SERVER={SQL_SERVER};\"\n",
    "        f\"DATABASE={db};\"\n",
    "        f\"Trusted_Connection=yes;\"\n",
    "    )\n",
    "    return create_engine(f\"mssql+pyodbc:///?odbc_connect={conn_str}\")\n",
    "\n",
    "cleaned_engine = get_engine(CLEANED_DB)\n",
    "\n",
    "# pyodbc connection\n",
    "odbc_conn = pyodbc.connect(\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={SQL_SERVER};\"\n",
    "    f\"DATABASE={CLEANED_DB};Trusted_Connection=yes;\"\n",
    ")\n",
    "cursor = odbc_conn.cursor()\n",
    "cursor.execute(\"\"\"\n",
    "IF OBJECT_ID('dbo.retrieveinfo','U') IS NULL\n",
    "CREATE TABLE dbo.retrieveinfo (\n",
    "    retrieve_id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "    source_file NVARCHAR(255),\n",
    "    retrieved_at DATETIME,\n",
    "    total_rows INT,\n",
    "    processed_rows INT,\n",
    "    errors INT,\n",
    "    notes NVARCHAR(500)\n",
    ")\n",
    "\"\"\")\n",
    "odbc_conn.commit()\n",
    "\n",
    "# Logging\n",
    "LOG_FILE = \"retrieveinfo_log.txt\"\n",
    "with open(LOG_FILE, \"w\", encoding=\"utf-8\") as log_f:\n",
    "    log_f.write(\"source_file,retrieved_at,total_rows,processed_rows,errors,notes\\n\")\n",
    "\n",
    "# Process each table\n",
    "for tid, table in TABLE_IDS.items():\n",
    "    csv_name = CSV_FILES[tid]\n",
    "    url = f\"{FILES_URL}/{csv_name}\"\n",
    "    print(f\"\\nDownloading {csv_name} ({table}) ...\")\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url); r.raise_for_status()\n",
    "        df_raw = pd.read_csv(io.StringIO(r.text))\n",
    "    except Exception as e:\n",
    "        print(f\"Download error: {e}\")\n",
    "        cursor.execute(\n",
    "            \"INSERT INTO retrieveinfo (source_file, retrieved_at, total_rows, processed_rows, errors, notes) VALUES (?,?,?,?,?,?)\",\n",
    "            csv_name, datetime.now(), 0, 0, 1, str(e)\n",
    "        )\n",
    "        odbc_conn.commit()\n",
    "        with open(LOG_FILE, \"a\", encoding=\"utf-8\") as log_f:\n",
    "            log_f.write(f\"{csv_name},{datetime.now().strftime('%Y-%m-%d %H:%M:%S')},0,0,1,error download\\n\")\n",
    "        continue\n",
    "\n",
    "    total_rows = len(df_raw)\n",
    "\n",
    "    cols = column_map[tid][\"columns\"]\n",
    "    rename_dict = {f\"{tid}-{k}\": v for k, v in cols.items()}\n",
    "    df = df_raw.rename(columns=rename_dict).copy()\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    if \"id\" not in df.columns:\n",
    "        df.insert(0, \"id\", range(1, len(df) + 1))\n",
    "\n",
    "    if \"is_vip\" in df:     \n",
    "        df[\"is_vip\"] = df[\"is_vip\"].astype(bool)\n",
    "    if \"is_blocked\" in df:  \n",
    "        df[\"is_blocked\"] = df[\"is_blocked\"].astype(bool)\n",
    "    for num in [\"amount\", \"total_balance\", \"balance\", \"limit_amount\",\n",
    "                \"total_transactions\", \"flagged_transactions\", \"total_amount\"]:\n",
    "        if num in df:\n",
    "            df[num] = pd.to_numeric(df[num], errors=\"coerce\").fillna(0)\n",
    "    for dt in df.columns:\n",
    "        if dt.endswith(\"_at\") or \"date\" in dt:\n",
    "            df[dt] = pd.to_datetime(df[dt], errors=\"coerce\")\n",
    "\n",
    "    # Save cleaned\n",
    "    clean_csv = f\"{table}.csv\"\n",
    "    df.to_csv(f\"{CLEANED_DIR}/{clean_csv}\", index=False)\n",
    "    df.to_sql(table, cleaned_engine, if_exists='replace', index=False)\n",
    "    print(f\"CLEANED -> {CLEANED_DB}.{table}\")\n",
    "\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO retrieveinfo (source_file, retrieved_at, total_rows, processed_rows, errors, notes) VALUES (?,?,?,?,?,?)\",\n",
    "        csv_name, datetime.now(), total_rows, len(df), 0, \"loaded\"\n",
    "    )\n",
    "    odbc_conn.commit()\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as log_f:\n",
    "        log_f.write(f\"{csv_name},{datetime.now().strftime('%Y-%m-%d %H:%M:%S')},{total_rows},{len(df)},0,loaded\\n\")\n",
    "\n",
    "# Derived tables\n",
    "print(\"\\nGenerating derived tables...\\n\")\n",
    "\n",
    "df_users = pd.read_csv(os.path.join(CLEANED_DIR, \"users.csv\"))\n",
    "df_cards = pd.read_csv(os.path.join(CLEANED_DIR, \"cards.csv\"))\n",
    "df_transactions = pd.read_csv(os.path.join(CLEANED_DIR, \"transactions.csv\"))\n",
    "\n",
    "# Fraud Detection\n",
    "fd = df_transactions.merge(\n",
    "    df_cards[[\"id\", \"limit_amount\", \"user_id\"]],\n",
    "    left_on=\"from_card_id\", right_on=\"id\",\n",
    "    suffixes=(\"_txn\", \"_card\")\n",
    ")\n",
    "fd = fd[fd[\"amount\"] > fd[\"limit_amount\"]].copy()\n",
    "fd[\"reason\"] = \"Amount exceeds card limit\"\n",
    "fd[\"status\"] = \"flagged\"\n",
    "\n",
    "fraud_out = pd.DataFrame()\n",
    "fraud_out[\"transaction_id\"] = fd[\"id_txn\"]\n",
    "fraud_out[\"from_card_id\"] = fd[\"from_card_id\"]\n",
    "fraud_out[\"user_id\"] = fd[\"user_id\"]\n",
    "fraud_out[\"reason\"] = fd[\"reason\"]\n",
    "fraud_out[\"status\"] = fd[\"status\"]\n",
    "fraud_out[\"created_at\"] = pd.to_datetime(fd[\"created_at\"], errors=\"coerce\")\n",
    "\n",
    "fraud_out.to_csv(f\"{CLEANED_DIR}/fraud_detection.csv\", index=False)\n",
    "fraud_out.to_sql(\"fraud_detection\", cleaned_engine, if_exists='replace', index=False)\n",
    "print(\"fraud_detection created\")\n",
    "\n",
    "# VIP Users\n",
    "df_users[\"total_balance\"] = pd.to_numeric(df_users[\"total_balance\"], errors=\"coerce\")\n",
    "vip = df_users[df_users[\"total_balance\"] > 5e8].copy()\n",
    "vip[\"assigned_at\"] = datetime.now()\n",
    "vip[\"reason\"] = \"High balance\"\n",
    "\n",
    "vip_out = vip[[\"id\", \"assigned_at\", \"reason\"]].copy()\n",
    "vip_out.columns = [\"user_id\", \"assigned_at\", \"reason\"]\n",
    "vip_out.to_csv(f\"{CLEANED_DIR}/vip_users.csv\", index=False)\n",
    "vip_out.to_sql(\"vip_users\", cleaned_engine, if_exists='replace', index=False)\n",
    "print(\"vip_users created\")\n",
    "\n",
    "# Blocked Users\n",
    "df_cards[\"balance\"] = pd.to_numeric(df_cards[\"balance\"], errors=\"coerce\")\n",
    "blk = df_cards[df_cards[\"balance\"] < 0].copy()\n",
    "blk[\"reason\"] = \"Negative balance\"\n",
    "blk[\"blocked_at\"] = datetime.now()\n",
    "\n",
    "blk_out = blk[[\"id\", \"reason\", \"blocked_at\"]].copy()\n",
    "blk_out.columns = [\"card_id\", \"reason\", \"blocked_at\"]\n",
    "blk_out.to_csv(f\"{CLEANED_DIR}/blocked_users.csv\", index=False)\n",
    "blk_out.to_sql(\"blocked_users\", cleaned_engine, if_exists='replace', index=False)\n",
    "print(\"blocked_users created\")\n",
    "\n",
    "cursor.close()\n",
    "odbc_conn.close()\n",
    "print(\"\\nPython is finished huurrray! :D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
