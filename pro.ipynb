{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Script 1 (Fresh Start): Ingestion, Decoding, Metadata --- START ---\n",
      "Ensured directories exist: 'raw_data', 'decoded_data'\n",
      "\n",
      "[Metadata Setup] Setting up connection...\n",
      "[Metadata Setup] Connection established.\n",
      "[Metadata Setup] Table check/creation completed.\n",
      "\n",
      "[Main] Loading column map from: https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/column_table_map.json\n",
      "[Main] JSON mapping successfully loaded and seems valid.\n",
      "\n",
      "[Main] Starting file ingestion and decoding loop...\n",
      "\n",
      ">>> Processing: t01.csv (Logical: users)\n",
      "  Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t01.csv...\n",
      "  Download complete.\n",
      "  Reading CSV...\n",
      "  Read complete: 600 rows, 8 columns.\n",
      "  Saving raw data to raw_data\\t01.csv...\n",
      "  Raw data saved.\n",
      "  Decoding columns...\n",
      "    Columns decoded/renamed: 8 (e.g., [('01-00', 'id'), ('01-01', 'name'), ('01-02', 'phone_number')]...)\n",
      "    Final columns: ['id', 'name', 'phone_number', 'email', 'created_at', 'last_active_at', 'is_vip', 'total_balance']\n",
      "  Decoding complete.\n",
      "  Storing DataFrame 'users' in memory.\n",
      "  Saving decoded data to decoded_data\\decoded_t01.csv...\n",
      "  Decoded data saved.\n",
      "  >>> Successfully processed t01.csv.\n",
      "  Logging metadata for t01.csv...\n",
      "  Metadata logged.\n",
      "\n",
      ">>> Processing: t02.csv (Logical: cards)\n",
      "  Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t02.csv...\n",
      "  Download complete.\n",
      "  Reading CSV...\n",
      "  Read complete: 1200 rows, 7 columns.\n",
      "  Saving raw data to raw_data\\t02.csv...\n",
      "  Raw data saved.\n",
      "  Decoding columns...\n",
      "    Columns decoded/renamed: 7 (e.g., [('02-00', 'id'), ('02-01', 'user_id'), ('02-02', 'card_number')]...)\n",
      "    Final columns: ['id', 'user_id', 'card_number', 'balance', 'created_at', 'card_type', 'limit_amount']\n",
      "  Decoding complete.\n",
      "  Storing DataFrame 'cards' in memory.\n",
      "  Saving decoded data to decoded_data\\decoded_t02.csv...\n",
      "  Decoded data saved.\n",
      "  >>> Successfully processed t02.csv.\n",
      "  Logging metadata for t02.csv...\n",
      "  Metadata logged.\n",
      "\n",
      ">>> Processing: t03.csv (Logical: transactions)\n",
      "  Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t03.csv...\n",
      "  Download complete.\n",
      "  Reading CSV...\n",
      "  Read complete: 3000 rows, 7 columns.\n",
      "  Saving raw data to raw_data\\t03.csv...\n",
      "  Raw data saved.\n",
      "  Decoding columns...\n",
      "    Columns decoded/renamed: 7 (e.g., [('03-00', 'id'), ('03-01', 'from_card_id'), ('03-02', 'to_card_id')]...)\n",
      "    Final columns: ['id', 'from_card_id', 'to_card_id', 'amount', 'status', 'created_at', 'transaction_type']\n",
      "  Decoding complete.\n",
      "  Storing DataFrame 'transactions' in memory.\n",
      "  Saving decoded data to decoded_data\\decoded_t03.csv...\n",
      "  Decoded data saved.\n",
      "  >>> Successfully processed t03.csv.\n",
      "  Logging metadata for t03.csv...\n",
      "  Metadata logged.\n",
      "\n",
      ">>> Processing: t04.csv (Logical: logs)\n",
      "  Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t04.csv...\n",
      "  Download complete.\n",
      "  Reading CSV...\n",
      "  Read complete: 2014 rows, 4 columns.\n",
      "  Saving raw data to raw_data\\t04.csv...\n",
      "  Raw data saved.\n",
      "  Decoding columns...\n",
      "    Columns decoded/renamed: 4 (e.g., [('04-00', 'id'), ('04-01', 'transaction_id'), ('04-02', 'message')]...)\n",
      "    Final columns: ['id', 'transaction_id', 'message', 'created_at']\n",
      "  Decoding complete.\n",
      "  Storing DataFrame 'logs' in memory.\n",
      "  Saving decoded data to decoded_data\\decoded_t04.csv...\n",
      "  Decoded data saved.\n",
      "  >>> Successfully processed t04.csv.\n",
      "  Logging metadata for t04.csv...\n",
      "  Metadata logged.\n",
      "\n",
      ">>> Processing: t05.csv (Logical: reports)\n",
      "  Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t05.csv...\n",
      "  Download complete.\n",
      "  Reading CSV...\n",
      "  Read complete: 92 rows, 6 columns.\n",
      "  Saving raw data to raw_data\\t05.csv...\n",
      "  Raw data saved.\n",
      "  Decoding columns...\n",
      "    Columns decoded/renamed: 6 (e.g., [('05-00', 'id'), ('05-01', 'report_type'), ('05-02', 'created_at')]...)\n",
      "    Final columns: ['id', 'report_type', 'created_at', 'total_transactions', 'flagged_transactions', 'total_amount']\n",
      "  Decoding complete.\n",
      "  Storing DataFrame 'reports' in memory.\n",
      "  Saving decoded data to decoded_data\\decoded_t05.csv...\n",
      "  Decoded data saved.\n",
      "  >>> Successfully processed t05.csv.\n",
      "  Logging metadata for t05.csv...\n",
      "  Metadata logged.\n",
      "\n",
      ">>> Processing: t07.csv (Logical: scheduled_payments)\n",
      "  Downloading from https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final/t07.csv...\n",
      "  Download complete.\n",
      "  Reading CSV...\n",
      "  Read complete: 800 rows, 7 columns.\n",
      "  Saving raw data to raw_data\\t07.csv...\n",
      "  Raw data saved.\n",
      "  Decoding columns...\n",
      "    Columns decoded/renamed: 7 (e.g., [('07-00', 'id'), ('07-01', 'user_id'), ('07-02', 'card_id')]...)\n",
      "    Final columns: ['id', 'user_id', 'card_id', 'amount', 'payment_date', 'status', 'created_at']\n",
      "  Decoding complete.\n",
      "  Storing DataFrame 'scheduled_payments' in memory.\n",
      "  Saving decoded data to decoded_data\\decoded_t07.csv...\n",
      "  Decoded data saved.\n",
      "  >>> Successfully processed t07.csv.\n",
      "  Logging metadata for t07.csv...\n",
      "  Metadata logged.\n",
      "\n",
      "[Main] Ingestion and decoding loop finished.\n",
      "DataFrames in memory: ['users', 'cards', 'transactions', 'logs', 'reports', 'scheduled_payments']\n",
      "Raw data location: 'raw_data'\n",
      "Decoded data location: 'decoded_data'\n",
      "\n",
      "[Metadata] Closing connection...\n",
      "[Metadata] Connection closed.\n",
      "\n",
      "--- Script 1 (Fresh Start): Ingestion, Decoding, Metadata --- END ---\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import pyodbc           # Metadata uchun\n",
    "import datetime         # Metadata uchun\n",
    "import urllib           # Connection string uchun\n",
    "\n",
    "print(\"--- Script 1 (Fresh Start): Ingestion, Decoding, Metadata --- START ---\")\n",
    "\n",
    "# --- Konfiguratsiya ---\n",
    "REPO_RAW_BASE = \"https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/files_final\"\n",
    "COLUMN_MAP_URL = \"https://raw.githubusercontent.com/odilbekmarimov/DemoProject/main/column_table_map.json\"\n",
    "\n",
    "file_mapping_config = {\n",
    "    't01.csv': {'logical_name': 'users', 'table_id': '01'},\n",
    "    't02.csv': {'logical_name': 'cards', 'table_id': '02'},\n",
    "    't03.csv': {'logical_name': 'transactions', 'table_id': '03'},\n",
    "    't04.csv': {'logical_name': 'logs', 'table_id': '04'},\n",
    "    't05.csv': {'logical_name': 'reports', 'table_id': '05'},\n",
    "    't07.csv': {'logical_name': 'scheduled_payments', 'table_id': '07'},\n",
    "}\n",
    "\n",
    "RAW_DIR = \"raw_data\"\n",
    "DECODED_DIR = \"decoded_data\"\n",
    "\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(DECODED_DIR, exist_ok=True)\n",
    "print(f\"Ensured directories exist: '{RAW_DIR}', '{DECODED_DIR}'\")\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "DB_SERVER_META = 'WIN-A5I5TM5OKQQ\\\\SQLEXPRESS'\n",
    "DB_DATABASE_META = 'BankingDB'\n",
    "USE_TRUSTED_CONNECTION_META = True\n",
    "driver_meta = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "create_metadata_table_sql = \"\"\"\n",
    "IF OBJECT_ID('dbo.retrieveinfo', 'U') IS NULL -- Jadval mavjudligini tekshirishning ishonchliroq usuli\n",
    "BEGIN\n",
    "    CREATE TABLE dbo.retrieveinfo (\n",
    "        retrieve_id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "        source_file NVARCHAR(100) NOT NULL,\n",
    "        retrieved_at DATETIME2 NOT NULL,\n",
    "        total_rows INT NULL,\n",
    "        processed_rows INT NULL,\n",
    "        errors INT DEFAULT 0,\n",
    "        notes NVARCHAR(MAX) NULL\n",
    "    );\n",
    "    PRINT '[Metadata Setup] Table retrieveinfo created successfully.';\n",
    "END\n",
    "ELSE\n",
    "BEGIN\n",
    "    PRINT '[Metadata Setup] Table retrieveinfo already exists.';\n",
    "END\n",
    "\"\"\"\n",
    "\n",
    "# --- Metadata uchun pyodbc Ulanish\n",
    "cnxn_meta = None\n",
    "cursor_meta = None\n",
    "print(\"\\n[Metadata Setup] Setting up connection...\")\n",
    "try:\n",
    "    if USE_TRUSTED_CONNECTION_META:\n",
    "        conn_str_meta = (f\"DRIVER={driver_meta};SERVER={DB_SERVER_META};\"\n",
    "                         f\"DATABASE={DB_DATABASE_META};Trusted_Connection=yes;\")\n",
    "    else:\n",
    "        pass\n",
    "    cnxn_meta = pyodbc.connect(conn_str_meta, autocommit=False)\n",
    "    cursor_meta = cnxn_meta.cursor()\n",
    "    print(\"[Metadata Setup] Connection established.\")\n",
    "    # Metadata jadvalini yaratish/tekshirish\n",
    "    cursor_meta.execute(create_metadata_table_sql)\n",
    "    cnxn_meta.commit()\n",
    "    print(\"[Metadata Setup] Table check/creation completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"[Metadata Setup] ERROR connecting or setting up table: {e}\")\n",
    "    print(\"[Metadata Setup] Metadata logging will be skipped.\")\n",
    "    cursor_meta = None; cnxn_meta = None\n",
    "\n",
    "# Column Mapni Yuklash\n",
    "print(f\"\\n[Main] Loading column map from: {COLUMN_MAP_URL}\")\n",
    "column_map_data = None\n",
    "try:\n",
    "    response_map = requests.get(COLUMN_MAP_URL)\n",
    "    response_map.raise_for_status()\n",
    "    column_map_data = response_map.json()\n",
    "    # JSON tuzilishini tekshirish (agar bo'sh bo'lsa yoki kutulmagan formatda bo'lsa)\n",
    "    if not isinstance(column_map_data, dict) or not column_map_data:\n",
    "        raise ValueError(\"JSON map is empty or not a valid dictionary.\")\n",
    "    print(\"[Main] JSON mapping successfully loaded and seems valid.\")\n",
    "except Exception as e:\n",
    "    print(f\"[Main] ERROR loading or parsing JSON mapping file: {e}\")\n",
    "    if cnxn_meta: cnxn_meta.close()\n",
    "    print(\"--- Script 1 (Fresh Start) --- FAILED ---\")\n",
    "    exit()\n",
    "\n",
    "#Fayllarni Yuklash, Dekodlash va Metadata Yozish\n",
    "print(\"\\n[Main] Starting file ingestion and decoding loop...\")\n",
    "for filename, info in file_mapping_config.items():\n",
    "    file_url = f\"{REPO_RAW_BASE}/{filename}\"\n",
    "    logical_name = info['logical_name']\n",
    "    table_id = info['table_id']\n",
    "\n",
    "    # Metadata o'zgaruvchilari\n",
    "    retrieval_start_time = datetime.datetime.now()\n",
    "    total_rows_read, processed_rows_count = None, None\n",
    "    error_flag, error_notes = 0, None\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    print(f\"\\n>>> Processing: {filename} (Logical: {logical_name})\")\n",
    "    try:\n",
    "        # Faylni yuklash\n",
    "        print(f\"  Downloading from {file_url}...\")\n",
    "        resp_csv = requests.get(file_url)\n",
    "        resp_csv.raise_for_status()\n",
    "        print(\"  Download complete.\")\n",
    "\n",
    "        # CSV ni o'qish\n",
    "        print(\"  Reading CSV...\")\n",
    "        try:\n",
    "            df = pd.read_csv(io.StringIO(resp_csv.text))\n",
    "        except UnicodeDecodeError:\n",
    "             print(\"    Warning: UTF-8 failed. Trying 'latin1'.\")\n",
    "             df = pd.read_csv(io.StringIO(resp_csv.content.decode('latin1')))\n",
    "        total_rows_read = len(df)\n",
    "        print(f\"  Read complete: {total_rows_read} rows, {len(df.columns)} columns.\")\n",
    "\n",
    "        # Xom faylni saqlash\n",
    "        raw_path = os.path.join(RAW_DIR, filename)\n",
    "        print(f\"  Saving raw data to {raw_path}...\")\n",
    "        with open(raw_path, 'wb') as f: f.write(resp_csv.content)\n",
    "        print(\"  Raw data saved.\")\n",
    "\n",
    "        # Ustunlarni dekodlash\n",
    "        print(\"  Decoding columns...\")\n",
    "        table_specific_map = column_map_data.get(table_id, {}).get(\"columns\", {})\n",
    "        if not table_specific_map:\n",
    "            print(f\"    Warning: No mapping found for table_id '{table_id}'. Keeping original names.\")\n",
    "            decoded_columns = df.columns.tolist()\n",
    "        else:\n",
    "            original_columns = df.columns.tolist()\n",
    "            decoded_columns = [table_specific_map.get(col.split(\"-\")[-1], col) for col in original_columns]\n",
    "            df.columns = decoded_columns\n",
    "            # Muvaffaqiyatli dekodlangan nomlarni chiqarish\n",
    "            changed_cols = {orig: dec for orig, dec in zip(original_columns, decoded_columns) if orig != dec}\n",
    "            print(f\"    Columns decoded/renamed: {len(changed_cols)} (e.g., {list(changed_cols.items())[:3]}...)\")\n",
    "            print(f\"    Final columns: {decoded_columns}\")\n",
    "        print(\"  Decoding complete.\")\n",
    "\n",
    "        processed_rows_count = len(df)\n",
    "\n",
    "        # Natijani saqlash\n",
    "        print(f\"  Storing DataFrame '{logical_name}' in memory.\")\n",
    "        dataframes[logical_name] = df\n",
    "        decoded_path = os.path.join(DECODED_DIR, f\"decoded_{filename}\")\n",
    "        print(f\"  Saving decoded data to {decoded_path}...\")\n",
    "        df.to_csv(decoded_path, index=False, encoding='utf-8')\n",
    "        print(\"  Decoded data saved.\")\n",
    "\n",
    "        error_flag = 0\n",
    "        error_notes = \"Success\"\n",
    "        print(f\"  >>> Successfully processed {filename}.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        error_flag = 1; error_notes = f\"Download Error: {e}\"\n",
    "        print(f\"  ERROR: {error_notes}\")\n",
    "        dataframes[logical_name] = pd.DataFrame()\n",
    "    except pd.errors.ParserError as e:\n",
    "        error_flag = 1; error_notes = f\"CSV Parsing Error: {e}\"\n",
    "        print(f\"  ERROR: {error_notes}\")\n",
    "        dataframes[logical_name] = pd.DataFrame()\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        error_flag = 0; error_notes = f\"File is empty: {e}\" # Bo'sh fayl xato emas\n",
    "        print(f\"  WARNING: {error_notes}\")\n",
    "        total_rows_read = 0; processed_rows_count = 0\n",
    "        dataframes[logical_name] = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        error_flag = 1; error_notes = f\"Unexpected Error: {e}\"\n",
    "        print(f\"  ERROR: {error_notes}\")\n",
    "        dataframes[logical_name] = pd.DataFrame()\n",
    "\n",
    "    # Metadata yozish\n",
    "    if cursor_meta and cnxn_meta:\n",
    "        print(f\"  Logging metadata for {filename}...\")\n",
    "        try:\n",
    "            sql_insert_meta = \"\"\"\n",
    "                INSERT INTO retrieveinfo (source_file, retrieved_at, total_rows, processed_rows, errors, notes)\n",
    "                VALUES (?, ?, ?, ?, ?, ?) \"\"\"\n",
    "            meta_data_tuple = (\n",
    "                filename, retrieval_start_time,\n",
    "                total_rows_read, processed_rows_count,\n",
    "                error_flag, str(error_notes)[:4000] if error_notes else None # Limit notes length\n",
    "            )\n",
    "            cursor_meta.execute(sql_insert_meta, meta_data_tuple)\n",
    "            cnxn_meta.commit()\n",
    "            print(\"  Metadata logged.\")\n",
    "        except Exception as meta_ex:\n",
    "            print(f\"  ERROR logging metadata: {meta_ex}\")\n",
    "            if cnxn_meta: cnxn_meta.rollback()\n",
    "    else:\n",
    "        print(\"  Skipping metadata logging (connection not available).\")\n",
    "\n",
    "    time.sleep(0.1) # Pauza\n",
    "\n",
    "# --- Yakunlash ---\n",
    "print(\"\\n[Main] Ingestion and decoding loop finished.\")\n",
    "print(f\"DataFrames in memory: {list(dataframes.keys())}\")\n",
    "print(f\"Raw data location: '{RAW_DIR}'\")\n",
    "print(f\"Decoded data location: '{DECODED_DIR}'\")\n",
    "\n",
    "if cnxn_meta:\n",
    "    print(\"\\n[Metadata] Closing connection...\")\n",
    "    if cursor_meta: cursor_meta.close()\n",
    "    cnxn_meta.close()\n",
    "    print(\"[Metadata] Connection closed.\")\n",
    "\n",
    "print(\"\\n--- Script 1 (Fresh Start): Ingestion, Decoding, Metadata --- END ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'dataframes' dictionary found in memory.\n",
      "\n",
      "--- Script 2 (Fresh Start): Data Cleaning and Transformation --- START ---\n",
      "\n",
      "[Main] Starting cleaning process for all DataFrames...\n",
      "\n",
      "--- Cleaning DataFrame: users ---\n",
      "Original shape: (600, 8)\n",
      "Shape after potential stripping: (600, 8)\n",
      "  Email valid count: 600 / 600\n",
      "  Phone valid count: 600 / 600\n",
      "  Shape after dropping duplicate 'id': (600, 10)\n",
      "\n",
      "--- Cleaning DataFrame: cards ---\n",
      "Original shape: (1200, 7)\n",
      "Shape after potential stripping: (1200, 7)\n",
      "  Shape after dropping duplicate 'id': (1200, 7)\n",
      "\n",
      "--- Cleaning DataFrame: transactions ---\n",
      "Original shape: (3000, 7)\n",
      "Shape after potential stripping: (3000, 7)\n",
      "  Flagged 2998 transactions > 15000.\n",
      "  Merging with cards on: Tx.'from_card_id' -> Cards.'id' for limit check.\n",
      "  Flagged 0 transactions exceeding card limit.\n",
      "  Shape after dropping duplicate 'id': (3000, 10)\n",
      "\n",
      "--- Cleaning DataFrame: logs ---\n",
      "Original shape: (2014, 4)\n",
      "Shape after potential stripping: (2014, 4)\n",
      "\n",
      "--- Cleaning DataFrame: reports ---\n",
      "Original shape: (92, 6)\n",
      "Shape after potential stripping: (92, 6)\n",
      "\n",
      "--- Cleaning DataFrame: scheduled_payments ---\n",
      "Original shape: (800, 7)\n",
      "Shape after potential stripping: (800, 7)\n",
      "\n",
      "[Main] Data cleaning and transformation process finished.\n",
      "  Cleaned 'users': (600, 10), Columns: ['id', 'name', 'phone_number', 'email', 'created_at', 'last_active_at', 'is_vip', 'total_balance', 'is_email_valid', 'is_phone_valid']\n",
      "  Cleaned 'cards': (1200, 7), Columns: ['id', 'user_id', 'card_number', 'balance', 'created_at', 'card_type', 'limit_amount']\n",
      "  Cleaned 'transactions': (3000, 10), Columns: ['id', 'from_card_id', 'to_card_id', 'amount', 'status', 'created_at', 'transaction_type', 'exceeds_transaction_threshold', 'exceeds_card_limit', 'card_limit']\n",
      "  Cleaned 'logs': (2014, 4), Columns: ['id', 'transaction_id', 'message', 'created_at']\n",
      "  Cleaned 'reports': (92, 6), Columns: ['id', 'report_type', 'created_at', 'total_transactions', 'flagged_transactions', 'total_amount']\n",
      "  Cleaned 'scheduled_payments': (800, 7), Columns: ['id', 'user_id', 'card_id', 'amount', 'payment_date', 'status', 'created_at']\n",
      "\n",
      "[Main] Saving cleaned data to 'cleaned_data' directory...\n",
      "  Saved: cleaned_data\\cleaned_users.csv\n",
      "  Saved: cleaned_data\\cleaned_cards.csv\n",
      "  Saved: cleaned_data\\cleaned_transactions.csv\n",
      "  Saved: cleaned_data\\cleaned_logs.csv\n",
      "  Saved: cleaned_data\\cleaned_reports.csv\n",
      "  Saved: cleaned_data\\cleaned_scheduled_payments.csv\n",
      "\n",
      "--- Script 2 (Fresh Start): Data Cleaning and Transformation --- END ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "if 'dataframes' not in locals() or not dataframes:\n",
    "    print(\"ERROR: 'dataframes' dictionary not found or empty in memory.\")\n",
    "    print(\"Please run Script 1 (Ingestion) first.\")\n",
    "    # Yoki decoded fayllardan yuklash logikasi\n",
    "    exit()\n",
    "else:\n",
    "     print(\"Using 'dataframes' dictionary found in memory.\")\n",
    "\n",
    "print(\"\\n--- Script 2 (Fresh Start): Data Cleaning and Transformation --- START ---\")\n",
    "\n",
    "# Tozalash Sozlamalari \n",
    "TRANSACTION_AMOUNT_THRESHOLD = 15000\n",
    "EMAIL_REGEX = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
    "PHONE_REGEX = r\"^(?=.*\\d{6,})[\\d\\s\\+\\-\\(\\)]+$\"\n",
    "\n",
    "cleaned_dataframes = {}\n",
    "\n",
    "def safe_to_datetime(series, **kwargs):\n",
    "    return pd.to_datetime(series, errors='coerce', **kwargs)\n",
    "def safe_to_numeric(series, **kwargs):\n",
    "    return pd.to_numeric(series, errors='coerce', **kwargs)\n",
    "def safe_to_boolean(series):\n",
    "    if series.dtype == 'bool': return series\n",
    "    map_dict = {'true': True, 'false': False, '1': True, '0': False, 1: True, 0: False, 'yes': True, 'no': False}\n",
    "    # Handle potential non-string values before lower()\n",
    "    return series.apply(lambda x: str(x).lower() if pd.notna(x) else x).map(map_dict).fillna(False).astype(bool)\n",
    "def safe_to_int64(series):\n",
    "     numeric_series = safe_to_numeric(series)\n",
    "     try: return numeric_series.astype('Int64')\n",
    "     except: return numeric_series # Agar Int64 bo'lmasa\n",
    "\n",
    "def clean_dataframe(df_orig, logical_name):\n",
    "    \"\"\" Umumiy tozalash \"\"\"\n",
    "    print(f\"\\n--- Cleaning DataFrame: {logical_name} ---\")\n",
    "    if df_orig is None or df_orig.empty:\n",
    "        print(\"DataFrame is empty or None. Skipping.\")\n",
    "        return pd.DataFrame()\n",
    "    df = df_orig.copy()\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    # String ustunlarni tozalash\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if col in df.columns and pd.api.types.is_string_dtype(df[col]):\n",
    "             try: df[col] = df[col].str.strip()\n",
    "             except: pass\n",
    "    print(f\"Shape after potential stripping: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_users(df_users):\n",
    "    df = clean_dataframe(df_users, 'users')\n",
    "    if df.empty: return df\n",
    "    # Ustun nomlari (Script 1 natijasidan)\n",
    "    id_col, created_at_col, last_active_col, is_vip_col, balance_col, email_col, phone_col = \\\n",
    "        'id', 'created_at', 'last_active_at', 'is_vip', 'total_balance', 'email', 'phone_number'\n",
    "\n",
    "    if id_col in df.columns: df[id_col] = safe_to_int64(df[id_col])\n",
    "    if created_at_col in df.columns: df[created_at_col] = safe_to_datetime(df[created_at_col])\n",
    "    if last_active_col in df.columns: df[last_active_col] = safe_to_datetime(df[last_active_col])\n",
    "    if is_vip_col in df.columns: df[is_vip_col] = safe_to_boolean(df[is_vip_col])\n",
    "    if balance_col in df.columns: df[balance_col] = safe_to_numeric(df[balance_col]).fillna(0)\n",
    "    if email_col in df.columns:\n",
    "        df['is_email_valid'] = df[email_col].apply(lambda x: bool(re.match(EMAIL_REGEX, str(x))) if pd.notna(x) else False)\n",
    "        print(f\"  Email valid count: {df['is_email_valid'].sum()} / {len(df)}\")\n",
    "    if phone_col in df.columns:\n",
    "        df['is_phone_valid'] = df[phone_col].apply(lambda x: bool(re.match(PHONE_REGEX, str(x))) if pd.notna(x) else False)\n",
    "        print(f\"  Phone valid count: {df['is_phone_valid'].sum()} / {len(df)}\")\n",
    "    # PK bo'yicha dublikatlarni o'chirish\n",
    "    if id_col in df.columns and not df[id_col].isnull().all():\n",
    "        df.drop_duplicates(subset=[id_col], keep='first', inplace=True)\n",
    "        print(f\"  Shape after dropping duplicate '{id_col}': {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def clean_cards(df_cards):\n",
    "    df = clean_dataframe(df_cards, 'cards')\n",
    "    if df.empty: return df\n",
    "    # Ustun nomlari (taxminiy - Skript 1 natijasini tekshiring!)\n",
    "    id_col, user_id_col, card_num_col, balance_col, limit_col, created_at_col, status_col = \\\n",
    "        'id', 'user_id', 'card_number', 'balance', 'limit_amount', 'created_at', 'status' # Agar status bo'lsa\n",
    "\n",
    "    if id_col in df.columns: df[id_col] = safe_to_int64(df[id_col])\n",
    "    if user_id_col in df.columns: df[user_id_col] = safe_to_int64(df[user_id_col])\n",
    "    if balance_col in df.columns: df[balance_col] = safe_to_numeric(df[balance_col]).fillna(0)\n",
    "    if limit_col in df.columns: df[limit_col] = safe_to_numeric(df[limit_col]).fillna(0)\n",
    "    if created_at_col in df.columns: df[created_at_col] = safe_to_datetime(df[created_at_col])\n",
    "    if status_col in df.columns: df[status_col] = df[status_col].astype(str).str.lower().str.strip()\n",
    "    # Card number odatda string bo'lishi kerak (boshida 0 bo'lishi mumkin)\n",
    "    if card_num_col in df.columns: df[card_num_col] = df[card_num_col].astype(str)\n",
    "\n",
    "    required_cols = [col for col in [id_col, user_id_col, card_num_col] if col in df.columns]\n",
    "    if required_cols: df.dropna(subset=required_cols, inplace=True)\n",
    "    # PK bo'yicha dublikatlar\n",
    "    if id_col in df.columns and not df[id_col].isnull().all():\n",
    "        df.drop_duplicates(subset=[id_col], keep='first', inplace=True)\n",
    "        print(f\"  Shape after dropping duplicate '{id_col}': {df.shape}\")\n",
    "    # Card number bo'yicha dublikatlar (agar u unique bo'lishi kerak bo'lsa)\n",
    "    if card_num_col in df.columns and not df[card_num_col].isnull().all():\n",
    "         orig_len = len(df)\n",
    "         df.drop_duplicates(subset=[card_num_col], keep='first', inplace=True)\n",
    "         if len(df) < orig_len: print(f\"  Shape after dropping duplicate '{card_num_col}': {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def clean_transactions(df_trans_orig, df_cards_cleaned):\n",
    "    df = clean_dataframe(df_trans_orig, 'transactions')\n",
    "    if df.empty: return df\n",
    "    # Ustun nomlari (taxminiy - Skript 1 natijasini tekshiring!)\n",
    "    id_col, from_card_col, to_card_col, amount_col, status_col, created_at_col = \\\n",
    "        'id', 'from_card_id', 'to_card_id', 'amount', 'status', 'created_at'\n",
    "    exceed_trans_thresh_col = 'exceeds_transaction_threshold'\n",
    "    exceed_card_limit_col = 'exceeds_card_limit'\n",
    "    card_limit_ref_col = 'card_limit' # Yangi ustun\n",
    "\n",
    "    if id_col in df.columns: df[id_col] = safe_to_int64(df[id_col])\n",
    "    # Karta ID lari Cards.id ga ishora qiladi deb faraz qilamiz (Int64)\n",
    "    if from_card_col in df.columns: df[from_card_col] = safe_to_int64(df[from_card_col])\n",
    "    if to_card_col in df.columns: df[to_card_col] = safe_to_int64(df[to_card_col])\n",
    "    if amount_col in df.columns: df[amount_col] = safe_to_numeric(df[amount_col]).fillna(0)\n",
    "    if status_col in df.columns: df[status_col] = df[status_col].astype(str).str.lower().str.strip()\n",
    "    if created_at_col in df.columns: df[created_at_col] = safe_to_datetime(df[created_at_col])\n",
    "\n",
    "    required_cols = [col for col in [id_col, from_card_col, amount_col, created_at_col] if col in df.columns]\n",
    "    if required_cols: df.dropna(subset=required_cols, inplace=True)\n",
    "\n",
    "    # Flagging\n",
    "    df[exceed_trans_thresh_col] = False\n",
    "    if amount_col in df.columns:\n",
    "         df[exceed_trans_thresh_col] = df[amount_col] > TRANSACTION_AMOUNT_THRESHOLD\n",
    "         print(f\"  Flagged {df[exceed_trans_thresh_col].sum()} transactions > {TRANSACTION_AMOUNT_THRESHOLD}.\")\n",
    "\n",
    "    df[exceed_card_limit_col] = False\n",
    "    df[card_limit_ref_col] = 0.0\n",
    "    if df_cards_cleaned is not None and not df_cards_cleaned.empty and from_card_col in df.columns:\n",
    "        # --- Karta limitini tekshirish (Cards.id bo'yicha) ---\n",
    "        card_id_in_cards = 'id'         # Cards PK\n",
    "        limit_col_in_cards = 'limit_amount' # Cards limit ustuni\n",
    "\n",
    "        if card_id_in_cards in df_cards_cleaned.columns and limit_col_in_cards in df_cards_cleaned.columns:\n",
    "            print(f\"  Merging with cards on: Tx.'{from_card_col}' -> Cards.'{card_id_in_cards}' for limit check.\")\n",
    "            cards_limits = df_cards_cleaned[[card_id_in_cards, limit_col_in_cards]].copy()\n",
    "            try:\n",
    "                df_merged = pd.merge(df, cards_limits, left_on=from_card_col, right_on=card_id_in_cards, how='left', suffixes=('', '_card'))\n",
    "                merged_limit_col = limit_col_in_cards # Agar nomlar bir xil bo'lsa\n",
    "                if limit_col_in_cards in df.columns: merged_limit_col += '_card' # Agar to'qnashuv bo'lsa\n",
    "\n",
    "                if merged_limit_col in df_merged.columns:\n",
    "                     df[card_limit_ref_col] = safe_to_numeric(df_merged[merged_limit_col]).fillna(0)\n",
    "                     if amount_col in df.columns:\n",
    "                           df[exceed_card_limit_col] = (df[amount_col] > df[card_limit_ref_col]) & (df[card_limit_ref_col] > 0)\n",
    "                           print(f\"  Flagged {df[exceed_card_limit_col].sum()} transactions exceeding card limit.\")\n",
    "                else: print(f\"  Warning: Limit column '{merged_limit_col}' not found after merge.\")\n",
    "            except Exception as e: print(f\"  ERROR during merge/limit flag: {e}\")\n",
    "        else: print(f\"  Warning: Required columns ('{card_id_in_cards}', '{limit_col_in_cards}') not found in cleaned cards.\")\n",
    "    else: print(\"  Info: Skipping card limit check (missing data).\")\n",
    "\n",
    "    # PK bo'yicha dublikatlar\n",
    "    if id_col in df.columns and not df[id_col].isnull().all():\n",
    "         df.drop_duplicates(subset=[id_col], keep='first', inplace=True)\n",
    "         print(f\"  Shape after dropping duplicate '{id_col}': {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Boshqa jadvallar uchun soddalashtirilgan funksiyalar\n",
    "def clean_logs(df_logs):\n",
    "    df = clean_dataframe(df_logs, 'logs')\n",
    "    if df.empty: return df\n",
    "    id_col, trans_id_col, created_at_col = 'id', 'transaction_id', 'created_at' # Taxminiy nomlar\n",
    "    if id_col in df.columns: df[id_col] = safe_to_int64(df[id_col])\n",
    "    if trans_id_col in df.columns: df[trans_id_col] = safe_to_int64(df[trans_id_col]) # Tx.id ga ishora qiladi\n",
    "    if created_at_col in df.columns: df[created_at_col] = safe_to_datetime(df[created_at_col])\n",
    "    if id_col in df.columns and not df[id_col].isnull().all(): df.drop_duplicates(subset=[id_col], keep='first', inplace=True)\n",
    "    return df\n",
    "\n",
    "def clean_reports(df_reports):\n",
    "    df = clean_dataframe(df_reports, 'reports')\n",
    "    if df.empty: return df\n",
    "    id_col, date_col, value_col = 'id', 'report_date', 'metric_value' # Taxminiy nomlar\n",
    "    if id_col in df.columns: df[id_col] = safe_to_int64(df[id_col])\n",
    "    if date_col in df.columns: df[date_col] = safe_to_datetime(df[date_col])\n",
    "    if value_col in df.columns: df[value_col] = safe_to_numeric(df[value_col]).fillna(0)\n",
    "    if id_col in df.columns and not df[id_col].isnull().all(): df.drop_duplicates(subset=[id_col], keep='first', inplace=True)\n",
    "    return df\n",
    "\n",
    "def clean_scheduled_payments(df_sched):\n",
    "    df = clean_dataframe(df_sched, 'scheduled_payments')\n",
    "    if df.empty: return df\n",
    "    id_col, from_card_col, amount_col, date_col, status_col = \\\n",
    "        'id', 'from_card_id', 'amount', 'scheduled_date', 'status' # Taxminiy nomlar\n",
    "    if id_col in df.columns: df[id_col] = safe_to_int64(df[id_col])\n",
    "    if from_card_col in df.columns: df[from_card_col] = safe_to_int64(df[from_card_col]) # Cards.id ga ishora qiladi\n",
    "    if amount_col in df.columns: df[amount_col] = safe_to_numeric(df[amount_col]).fillna(0)\n",
    "    if date_col in df.columns: df[date_col] = safe_to_datetime(df[date_col])\n",
    "    if status_col in df.columns: df[status_col] = df[status_col].astype(str).str.lower().str.strip()\n",
    "    required_cols = [col for col in [id_col, from_card_col, amount_col, date_col] if col in df.columns]\n",
    "    if required_cols: df.dropna(subset=required_cols, inplace=True)\n",
    "    if id_col in df.columns and not df[id_col].isnull().all(): df.drop_duplicates(subset=[id_col], keep='first', inplace=True)\n",
    "    return df\n",
    "\n",
    "#Asosiy Tozalash Jarayonini Bajarish\n",
    "print(\"\\n[Main] Starting cleaning process for all DataFrames...\")\n",
    "\n",
    "# Har bir DataFrame uchun tozalash funksiyasini chaqirish\n",
    "# .get() xavfsizlik uchun ishlatiladi (agar DF yuklanmagan bo'lsa)\n",
    "cleaned_dataframes['users'] = clean_users(dataframes.get('users'))\n",
    "cleaned_dataframes['cards'] = clean_cards(dataframes.get('cards'))\n",
    "# Tranzaksiyalarni tozalash uchun TOZALANGAN kartalar kerak\n",
    "cleaned_dataframes['transactions'] = clean_transactions(\n",
    "                                        dataframes.get('transactions'),\n",
    "                                        cleaned_dataframes.get('cards')\n",
    "                                     )\n",
    "cleaned_dataframes['logs'] = clean_logs(dataframes.get('logs'))\n",
    "cleaned_dataframes['reports'] = clean_reports(dataframes.get('reports'))\n",
    "cleaned_dataframes['scheduled_payments'] = clean_scheduled_payments(dataframes.get('scheduled_payments'))\n",
    "\n",
    "print(\"\\n[Main] Data cleaning and transformation process finished.\")\n",
    "# Tozalangan DFlar haqida qisqacha ma'lumot\n",
    "for name, df_clean in cleaned_dataframes.items():\n",
    "    if df_clean is not None and not df_clean.empty:\n",
    "        print(f\"  Cleaned '{name}': {df_clean.shape}, Columns: {df_clean.columns.tolist()}\")\n",
    "    else:\n",
    "        print(f\"  Cleaned '{name}': Empty or None\")\n",
    "\n",
    "# --- Ixtiyoriy: Tozalangan ma'lumotlarni faylga saqlash ---\n",
    "CLEANED_DIR = \"cleaned_data\"\n",
    "os.makedirs(CLEANED_DIR, exist_ok=True)\n",
    "print(f\"\\n[Main] Saving cleaned data to '{CLEANED_DIR}' directory...\")\n",
    "for name, df in cleaned_dataframes.items():\n",
    "    if df is not None and not df.empty:\n",
    "        try:\n",
    "            save_path = os.path.join(CLEANED_DIR, f\"cleaned_{name}.csv\")\n",
    "            # Sana formatini saqlashda muammo bo'lmasligi uchun\n",
    "            df.to_csv(save_path, index=False, encoding='utf-8', date_format='%Y-%m-%d %H:%M:%S.%f')\n",
    "            print(f\"  Saved: {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error saving cleaned DataFrame '{name}': {e}\")\n",
    "    else:\n",
    "        print(f\"  Skipping saving for '{name}'.\")\n",
    "\n",
    "print(\"\\n--- Script 2 (Fresh Start): Data Cleaning and Transformation --- END ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TEST SCRIPT (NumPy Fix): Load ONLY Users Table --- START ---\n",
      "Using 'cleaned_dataframes['users']'. Shape: (600, 10)\n",
      "\n",
      "[Test Load] Setting up connection (AUTOCOMMIT=FALSE)...\n",
      "[Test Load] Connection established.\n",
      "\n",
      "[Test Load] STEP 0: Dropping table 'Users' if exists...\n",
      "  Table 'Users' dropped. Commit successful.\n",
      "\n",
      "[Test Load] STEP 1a: Creating table 'Users'...\n",
      "  Table 'Users' created. Commit successful.\n",
      "\n",
      "[Test Load] STEP 1b: Preparing and inserting 600 rows...\n",
      "  Executing INSERT statement for 600 rows...\n",
      "  SUCCESS: Inserted -1 rows. Commit successful.\n",
      "\n",
      "[Test Load] Successfully processed 'Users'.\n",
      "\n",
      "[Test Load] Closing connection...\n",
      "[Test Load] Connection closed.\n",
      "\n",
      "--- TEST SCRIPT (NumPy Fix): Load ONLY Users Table --- END ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib\n",
    "import datetime # Datetime konvertatsiyasi uchun\n",
    "\n",
    "print(\"--- TEST SCRIPT (NumPy Fix): Load ONLY Users Table --- START ---\")\n",
    "\n",
    "if 'cleaned_dataframes' not in locals() or not cleaned_dataframes.get('users') is not None and cleaned_dataframes.get('users').empty:\n",
    "    print(\"ERROR: 'cleaned_dataframes['users']' not found or empty. Run Script 1 & 2 first.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(\"Using 'cleaned_dataframes['users']'. Shape:\", cleaned_dataframes['users'].shape)\n",
    "\n",
    "DB_SERVER = 'WIN-A5I5TM5OKQQ\\\\SQLEXPRESS'\n",
    "DB_DATABASE = 'BankingDB'\n",
    "USE_TRUSTED_CONNECTION = True\n",
    "driver = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "cnxn = None\n",
    "cursor = None\n",
    "print(\"\\n[Test Load] Setting up connection (AUTOCOMMIT=FALSE)...\")\n",
    "try:\n",
    "    conn_str = (f\"DRIVER={driver};SERVER={DB_SERVER};\"\n",
    "                 f\"DATABASE={DB_DATABASE};Trusted_Connection=yes;\")\n",
    "    cnxn = pyodbc.connect(conn_str, autocommit=False)\n",
    "    cursor = cnxn.cursor()\n",
    "    print(\"[Test Load] Connection established.\")\n",
    "except Exception as e:\n",
    "    print(f\"[Test Load] ERROR connecting: {e}\")\n",
    "    exit()\n",
    "\n",
    " SQL Type Function\n",
    "def get_sql_type(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype): return 'BIGINT'\n",
    "    elif pd.api.types.is_float_dtype(dtype): return 'FLOAT'\n",
    "    elif pd.api.types.is_datetime64_any_dtype(dtype): return 'DATETIME2'\n",
    "    elif pd.api.types.is_bool_dtype(dtype): return 'BIT'\n",
    "    else: return 'NVARCHAR(MAX)'\n",
    "\n",
    "logical_name = 'users'\n",
    "final_table_name = 'Users'\n",
    "df_cleaned = cleaned_dataframes[logical_name]\n",
    "pk_col = 'id'\n",
    "\n",
    "try:\n",
    "    print(f\"\\n[Test Load] STEP 0: Dropping table '{final_table_name}' if exists...\")\n",
    "    try:\n",
    "        cursor.execute(f\"DROP TABLE IF EXISTS [{final_table_name}]\")\n",
    "        cnxn.commit()\n",
    "        print(f\"  Table '{final_table_name}' dropped. Commit successful.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR dropping table '{final_table_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "    print(f\"\\n[Test Load] STEP 1a: Creating table '{final_table_name}'...\")\n",
    "    column_defs = []\n",
    "    valid_sql_cols = []\n",
    "    for col_name in df_cleaned.columns:\n",
    "        if not isinstance(col_name, str) or not col_name: continue\n",
    "        sql_col_name = col_name\n",
    "        sql_type = get_sql_type(df_cleaned[col_name].dtype)\n",
    "        column_defs.append(f\"[{sql_col_name}] {sql_type}\")\n",
    "        valid_sql_cols.append(sql_col_name)\n",
    "    pk_def = f\", CONSTRAINT PK_{final_table_name} PRIMARY KEY ([{pk_col}])\" if pk_col in valid_sql_cols else \"\"\n",
    "    create_sql = f\"CREATE TABLE [{final_table_name}] ({', '.join(column_defs)} {pk_def} )\"\n",
    "    cursor.execute(create_sql)\n",
    "    cnxn.commit()\n",
    "    print(f\"  Table '{final_table_name}' created. Commit successful.\")\n",
    "\n",
    "    print(f\"\\n[Test Load] STEP 1b: Preparing and inserting {len(df_cleaned)} rows...\")\n",
    "    sql_cols_str = ', '.join([f'[{c}]' for c in valid_sql_cols])\n",
    "    placeholders = ', '.join('?' * len(valid_sql_cols))\n",
    "    sql_insert = f\"INSERT INTO [{final_table_name}] ({sql_cols_str}) VALUES ({placeholders})\"\n",
    "\n",
    "    data_tuples = []\n",
    "    for row in df_cleaned[valid_sql_cols].itertuples(index=False, name=None):\n",
    "        processed_row = []\n",
    "        for item in row:\n",
    "            if pd.isna(item): # Check for NaN, NaT, pd.NA\n",
    "                processed_row.append(None)\n",
    "            elif isinstance(item, (np.int64, np.int32, np.int16, np.int8)):\n",
    "                processed_row.append(int(item)) # NumPy int -> Python int\n",
    "            elif isinstance(item, (np.float64, np.float32, np.float16)):\n",
    "                processed_row.append(float(item)) # NumPy float -> Python float\n",
    "            elif isinstance(item, (np.bool_)):\n",
    "                 processed_row.append(bool(item)) # NumPy bool -> Python bool\n",
    "\n",
    "            else:\n",
    "                processed_row.append(item) # Qolgan turlarni o'zgartirmaymiz\n",
    "        data_tuples.append(tuple(processed_row))\n",
    "    # --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n",
    "\n",
    "    if not data_tuples:\n",
    "        print(\"    Warning: No data tuples to insert.\")\n",
    "    else:\n",
    "        print(f\"  Executing INSERT statement for {len(data_tuples)} rows...\")\n",
    "        cursor.fast_executemany = True\n",
    "        cursor.executemany(sql_insert, data_tuples)\n",
    "        inserted_rows = cursor.rowcount\n",
    "        cnxn.commit() # <<< COMMIT after successful INSERT\n",
    "        print(f\"  SUCCESS: Inserted {inserted_rows} rows. Commit successful.\")\n",
    "\n",
    "    print(f\"\\n[Test Load] Successfully processed '{final_table_name}'.\")\n",
    "\n",
    "except pyodbc.Error as db_e: # Faqat pyodbc xatolarini ushlash\n",
    "    print(f\"\\n[Test Load] A DATABASE error occurred: {db_e}\")\n",
    "    print(f\"  SQL State: {db_e.args[0]}\")\n",
    "    print(\"Attempting to rollback...\")\n",
    "    if cnxn: cnxn.rollback()\n",
    "    print(\"Rollback attempted.\")\n",
    "except Exception as e: # Boshqa Python xatolari\n",
    "    print(f\"\\n[Test Load] An unexpected PYTHON error occurred: {e}\")\n",
    "    print(\"Attempting to rollback...\")\n",
    "    if cnxn: cnxn.rollback()\n",
    "    print(\"Rollback attempted.\")\n",
    "finally:\n",
    "    print(\"\\n[Test Load] Closing connection...\")\n",
    "    if cursor: cursor.close()\n",
    "    if cnxn: cnxn.close()\n",
    "    print(\"[Test Load] Connection closed.\")\n",
    "\n",
    "print(\"\\n--- TEST SCRIPT (NumPy Fix): Load ONLY Users Table --- END ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3C (NumPy 2.0 Fix): Load Table 'Cards' --- START ---\n",
      "Processing 'Cards' with 1200 rows.\n",
      "  Connection established.\n",
      "  Creating table 'Cards'...\n",
      "  Table 'Cards' created.\n",
      "  Preparing and inserting rows...\n",
      "  SUCCESS: Inserted -1 rows.\n",
      "  Changes for 'Cards' committed.\n",
      "  Connection closed.\n",
      "--- Step 3C (NumPy 2.0 Fix): Load Table 'Cards' --- END ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import numpy as np # NumPy import qilingan\n",
    "import urllib\n",
    "import datetime\n",
    "\n",
    "print(\"\\n--- Step 3C (NumPy 2.0 Fix): Load Table 'Cards' --- START ---\")\n",
    "logical_name = 'cards'; final_table_name = 'Cards'; pk_col = 'id'\n",
    "card_number_col = 'card_number'\n",
    "\n",
    "if 'cleaned_dataframes' not in locals() or cleaned_dataframes.get(logical_name) is None or cleaned_dataframes.get(logical_name).empty:\n",
    "    print(f\"ERROR/SKIP: cleaned_dataframes['{logical_name}'] not found or empty.\")\n",
    "else:\n",
    "    df_cleaned = cleaned_dataframes[logical_name]\n",
    "    print(f\"Processing '{final_table_name}' with {len(df_cleaned)} rows.\")\n",
    "    DB_SERVER = 'WIN-A5I5TM5OKQQ\\\\SQLEXPRESS'; DB_DATABASE = 'BankingDB'; USE_TRUSTED_CONNECTION = True; driver = '{ODBC Driver 17 for SQL Server}'\n",
    "    cnxn = None; cursor = None\n",
    "\n",
    "    # --- SQL Type Function ---\n",
    "    def get_sql_type(column_name, dtype):\n",
    "        if column_name == card_number_col: return 'NVARCHAR(50)'\n",
    "        elif pd.api.types.is_integer_dtype(dtype): return 'BIGINT'\n",
    "        elif pd.api.types.is_float_dtype(dtype): return 'FLOAT'\n",
    "        elif pd.api.types.is_datetime64_any_dtype(dtype): return 'DATETIME2'\n",
    "        elif pd.api.types.is_bool_dtype(dtype): return 'BIT'\n",
    "        else: return 'NVARCHAR(MAX)'\n",
    "\n",
    "    def convert_value(item):\n",
    "        if pd.isna(item): return None\n",
    "        # Kengroq tekshiruv NumPy 2.0+ uchun\n",
    "        if isinstance(item, (np.integer)): # Barcha NumPy integer turlari uchun\n",
    "            return int(item)\n",
    "        if isinstance(item, (np.floating)): # Barcha NumPy float turlari uchun\n",
    "            return float(item)\n",
    "        if isinstance(item, (np.bool_)): # NumPy boolean uchun (pastki chiziq bilan)\n",
    "            return bool(item)\n",
    "        # Pandas Timestamp ni ham o'girish (agar kerak bo'lsa)\n",
    "        # if isinstance(item, pd.Timestamp): return item.to_pydatetime()\n",
    "        return item # Qolganlarini o'zgartirmaymiz\n",
    "\n",
    "    try:\n",
    "        conn_str = (f\"DRIVER={driver};SERVER={DB_SERVER};DATABASE={DB_DATABASE};Trusted_Connection=yes;\")\n",
    "        cnxn = pyodbc.connect(conn_str, autocommit=False)\n",
    "        cursor = cnxn.cursor()\n",
    "        print(\"  Connection established.\")\n",
    "\n",
    "        # CREATE TABLE\n",
    "        print(f\"  Creating table '{final_table_name}'...\")\n",
    "        column_defs, valid_sql_cols = [], []\n",
    "        for col_name in df_cleaned.columns:\n",
    "            if not isinstance(col_name, str) or not col_name: continue\n",
    "            sql_col_name=col_name\n",
    "            sql_type=get_sql_type(sql_col_name, df_cleaned[col_name].dtype)\n",
    "            column_defs.append(f\"[{sql_col_name}] {sql_type}\"); valid_sql_cols.append(sql_col_name)\n",
    "        pk_def = f\", CONSTRAINT PK_{final_table_name} PRIMARY KEY ([{pk_col}])\" if pk_col in valid_sql_cols else \"\"\n",
    "        unique_card_def = f\", CONSTRAINT UQ_{final_table_name}_CardNumber UNIQUE ([{card_number_col}])\" if card_number_col in valid_sql_cols else \"\"\n",
    "        create_sql = f\"CREATE TABLE [{final_table_name}] ({', '.join(column_defs)} {pk_def} {unique_card_def} )\"\n",
    "        cursor.execute(f\"DROP TABLE IF EXISTS [{final_table_name}]\")\n",
    "        cursor.execute(create_sql); print(f\"  Table '{final_table_name}' created.\")\n",
    "\n",
    "        # INSERT INTO (YANGILANGAN convert_value bilan)\n",
    "        print(f\"  Preparing and inserting rows...\")\n",
    "        sql_cols_str = ', '.join([f'[{c}]' for c in valid_sql_cols])\n",
    "        placeholders = ', '.join('?' * len(valid_sql_cols))\n",
    "        sql_insert = f\"INSERT INTO [{final_table_name}] ({sql_cols_str}) VALUES ({placeholders})\"\n",
    "        # >>> data_tuples endi yangi convert_value ni ishlatadi <<<\n",
    "        data_tuples = [tuple(convert_value(item) for item in row) for row in df_cleaned[valid_sql_cols].itertuples(index=False, name=None)]\n",
    "        if data_tuples:\n",
    "            cursor.fast_executemany = True; cursor.executemany(sql_insert, data_tuples)\n",
    "            print(f\"  SUCCESS: Inserted {cursor.rowcount} rows.\")\n",
    "        else: print(\"    Warning: No data to insert.\")\n",
    "        cnxn.commit(); print(f\"  Changes for '{final_table_name}' committed.\")\n",
    "\n",
    "    except Exception as e: print(f\"  ERROR processing table '{final_table_name}': {e}\"); cnxn.rollback()\n",
    "    finally:\n",
    "        if cursor: cursor.close()\n",
    "        if cnxn: cnxn.close()\n",
    "        print(\"  Connection closed.\")\n",
    "print(\"--- Step 3C (NumPy 2.0 Fix): Load Table 'Cards' --- END ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3D: Load Table 'Transactions' --- START ---\n",
      "Processing 'Transactions' with 3000 rows.\n",
      "  Connection established.\n",
      "  Creating table 'Transactions'...\n",
      "  Table 'Transactions' created.\n",
      "  Preparing and inserting rows...\n",
      "  SUCCESS: Inserted -1 rows.\n",
      "  Changes for 'Transactions' committed.\n",
      "  Connection closed.\n",
      "--- Step 3D: Load Table 'Transactions' --- END ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "import urllib\n",
    "import datetime\n",
    "\n",
    "print(\"\\n--- Step 3D: Load Table 'Transactions' --- START ---\")\n",
    "logical_name = 'transactions'; final_table_name = 'Transactions'; pk_col = 'id'\n",
    "\n",
    "if 'cleaned_dataframes' not in locals() or cleaned_dataframes.get(logical_name) is None or cleaned_dataframes.get(logical_name).empty:\n",
    "    print(f\"ERROR/SKIP: cleaned_dataframes['{logical_name}'] not found or empty.\")\n",
    "else:\n",
    "    df_cleaned = cleaned_dataframes[logical_name]\n",
    "    print(f\"Processing '{final_table_name}' with {len(df_cleaned)} rows.\")\n",
    "    DB_SERVER = 'WIN-A5I5TM5OKQQ\\\\SQLEXPRESS'; DB_DATABASE = 'BankingDB'; USE_TRUSTED_CONNECTION = True; driver = '{ODBC Driver 17 for SQL Server}'\n",
    "    cnxn = None; cursor = None\n",
    "    # --- SQL Type Function ---\n",
    "    def get_sql_type(dtype): # Column name not needed here unless specific override\n",
    "        if pd.api.types.is_integer_dtype(dtype): return 'BIGINT'\n",
    "        elif pd.api.types.is_float_dtype(dtype): return 'FLOAT'\n",
    "        elif pd.api.types.is_datetime64_any_dtype(dtype): return 'DATETIME2'\n",
    "        elif pd.api.types.is_bool_dtype(dtype): return 'BIT'\n",
    "        else: return 'NVARCHAR(MAX)'\n",
    "    # --- NumPy/Pandas ni Python ga O'girish Funksiyasi ---\n",
    "    def convert_value(item):\n",
    "        if pd.isna(item): return None\n",
    "        if isinstance(item, (np.integer)): return int(item)\n",
    "        if isinstance(item, (np.floating)): return float(item)\n",
    "        if isinstance(item, (np.bool_)): return bool(item)\n",
    "        return item\n",
    "    try:\n",
    "        conn_str = (f\"DRIVER={driver};SERVER={DB_SERVER};DATABASE={DB_DATABASE};Trusted_Connection=yes;\")\n",
    "        cnxn = pyodbc.connect(conn_str, autocommit=False)\n",
    "        cursor = cnxn.cursor()\n",
    "        print(\"  Connection established.\")\n",
    "        # CREATE TABLE\n",
    "        print(f\"  Creating table '{final_table_name}'...\")\n",
    "        column_defs, valid_sql_cols = [], []\n",
    "        for col_name in df_cleaned.columns:\n",
    "            if not isinstance(col_name, str) or not col_name: continue\n",
    "            sql_col_name=col_name; sql_type=get_sql_type(df_cleaned[col_name].dtype)\n",
    "            column_defs.append(f\"[{sql_col_name}] {sql_type}\"); valid_sql_cols.append(sql_col_name)\n",
    "        pk_def = f\", CONSTRAINT PK_{final_table_name} PRIMARY KEY ([{pk_col}])\" if pk_col in valid_sql_cols else \"\"\n",
    "        create_sql = f\"CREATE TABLE [{final_table_name}] ({', '.join(column_defs)} {pk_def} )\" # Removed UNIQUE constraint\n",
    "        cursor.execute(f\"DROP TABLE IF EXISTS [{final_table_name}]\")\n",
    "        cursor.execute(create_sql); print(f\"  Table '{final_table_name}' created.\")\n",
    "        # INSERT INTO\n",
    "        print(f\"  Preparing and inserting rows...\")\n",
    "        sql_cols_str = ', '.join([f'[{c}]' for c in valid_sql_cols])\n",
    "        placeholders = ', '.join('?' * len(valid_sql_cols))\n",
    "        sql_insert = f\"INSERT INTO [{final_table_name}] ({sql_cols_str}) VALUES ({placeholders})\"\n",
    "        data_tuples = [tuple(convert_value(item) for item in row) for row in df_cleaned[valid_sql_cols].itertuples(index=False, name=None)]\n",
    "        if data_tuples:\n",
    "            cursor.fast_executemany = True; cursor.executemany(sql_insert, data_tuples)\n",
    "            print(f\"  SUCCESS: Inserted {cursor.rowcount} rows.\")\n",
    "        else: print(\"    Warning: No data to insert.\")\n",
    "        cnxn.commit(); print(f\"  Changes for '{final_table_name}' committed.\")\n",
    "    except Exception as e: print(f\"  ERROR processing table '{final_table_name}': {e}\"); cnxn.rollback()\n",
    "    finally:\n",
    "        if cursor: cursor.close()\n",
    "        if cnxn: cnxn.close()\n",
    "        print(\"  Connection closed.\")\n",
    "print(f\"--- Step 3D: Load Table '{final_table_name}' --- END ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3E: Load Table 'Logs' --- START ---\n",
      "Processing 'Logs' with 2014 rows.\n",
      "  Connection established.\n",
      "  Creating table 'Logs'...\n",
      "  Table 'Logs' created.\n",
      "  Preparing and inserting rows...\n",
      "  SUCCESS: Inserted -1 rows.\n",
      "  Changes for 'Logs' committed.\n",
      "  Connection closed.\n",
      "--- Step 3E: Load Table 'Logs' --- END ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "import urllib\n",
    "import datetime\n",
    "\n",
    "print(\"\\n--- Step 3E: Load Table 'Logs' --- START ---\")\n",
    "logical_name = 'logs'; final_table_name = 'Logs'; pk_col = 'id' # <<< Moslashtiring\n",
    "\n",
    "if 'cleaned_dataframes' not in locals() or cleaned_dataframes.get(logical_name) is None or cleaned_dataframes.get(logical_name).empty:\n",
    "    print(f\"ERROR/SKIP: cleaned_dataframes['{logical_name}'] not found or empty.\")\n",
    "else:\n",
    "    df_cleaned = cleaned_dataframes[logical_name]\n",
    "    print(f\"Processing '{final_table_name}' with {len(df_cleaned)} rows.\")\n",
    "    DB_SERVER = 'WIN-A5I5TM5OKQQ\\\\SQLEXPRESS'; DB_DATABASE = 'BankingDB'; USE_TRUSTED_CONNECTION = True; driver = '{ODBC Driver 17 for SQL Server}'\n",
    "    cnxn = None; cursor = None\n",
    "    def get_sql_type(dtype):\n",
    "        if pd.api.types.is_integer_dtype(dtype): return 'BIGINT'\n",
    "        elif pd.api.types.is_float_dtype(dtype): return 'FLOAT'\n",
    "        elif pd.api.types.is_datetime64_any_dtype(dtype): return 'DATETIME2'\n",
    "        elif pd.api.types.is_bool_dtype(dtype): return 'BIT'\n",
    "        else: return 'NVARCHAR(MAX)'\n",
    "    def convert_value(item):\n",
    "        if pd.isna(item): return None\n",
    "        if isinstance(item, (np.integer)): return int(item)\n",
    "        if isinstance(item, (np.floating)): return float(item)\n",
    "        if isinstance(item, (np.bool_)): return bool(item)\n",
    "        return item\n",
    "    try:\n",
    "        conn_str = (f\"DRIVER={driver};SERVER={DB_SERVER};DATABASE={DB_DATABASE};Trusted_Connection=yes;\")\n",
    "        cnxn = pyodbc.connect(conn_str, autocommit=False)\n",
    "        cursor = cnxn.cursor()\n",
    "        print(\"  Connection established.\")\n",
    "        # CREATE TABLE\n",
    "        print(f\"  Creating table '{final_table_name}'...\")\n",
    "        column_defs, valid_sql_cols = [], []\n",
    "        for col_name in df_cleaned.columns:\n",
    "            if not isinstance(col_name, str) or not col_name: continue\n",
    "            sql_col_name=col_name; sql_type=get_sql_type(df_cleaned[col_name].dtype)\n",
    "            column_defs.append(f\"[{sql_col_name}] {sql_type}\"); valid_sql_cols.append(sql_col_name)\n",
    "        pk_def = f\", CONSTRAINT PK_{final_table_name} PRIMARY KEY ([{pk_col}])\" if pk_col in valid_sql_cols else \"\"\n",
    "        create_sql = f\"CREATE TABLE [{final_table_name}] ({', '.join(column_defs)} {pk_def} )\"\n",
    "        cursor.execute(f\"DROP TABLE IF EXISTS [{final_table_name}]\")\n",
    "        cursor.execute(create_sql); print(f\"  Table '{final_table_name}' created.\")\n",
    "        # INSERT INTO\n",
    "        print(f\"  Preparing and inserting rows...\")\n",
    "        sql_cols_str = ', '.join([f'[{c}]' for c in valid_sql_cols])\n",
    "        placeholders = ', '.join('?' * len(valid_sql_cols))\n",
    "        sql_insert = f\"INSERT INTO [{final_table_name}] ({sql_cols_str}) VALUES ({placeholders})\"\n",
    "        data_tuples = [tuple(convert_value(item) for item in row) for row in df_cleaned[valid_sql_cols].itertuples(index=False, name=None)]\n",
    "        if data_tuples:\n",
    "            cursor.fast_executemany = True; cursor.executemany(sql_insert, data_tuples)\n",
    "            print(f\"  SUCCESS: Inserted {cursor.rowcount} rows.\")\n",
    "        else: print(\"    Warning: No data to insert.\")\n",
    "        cnxn.commit(); print(f\"  Changes for '{final_table_name}' committed.\")\n",
    "    except Exception as e: print(f\"  ERROR processing table '{final_table_name}': {e}\"); cnxn.rollback()\n",
    "    finally:\n",
    "        if cursor: cursor.close()\n",
    "        if cnxn: cnxn.close()\n",
    "        print(\"  Connection closed.\")\n",
    "print(f\"--- Step 3E: Load Table '{final_table_name}' --- END ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3G: Load Table 'ScheduledPayments' --- START ---\n",
      "Processing 'ScheduledPayments' with 800 rows.\n",
      "  Connection established.\n",
      "  Creating table 'ScheduledPayments'...\n",
      "  Table 'ScheduledPayments' created.\n",
      "  Preparing and inserting rows...\n",
      "  SUCCESS: Inserted -1 rows.\n",
      "  Changes for 'ScheduledPayments' committed.\n",
      "  Connection closed.\n",
      "--- Step 3G: Load Table 'ScheduledPayments' --- END ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "import urllib\n",
    "import datetime\n",
    "\n",
    "print(\"\\n--- Step 3G: Load Table 'ScheduledPayments' --- START ---\")\n",
    "logical_name = 'scheduled_payments'; final_table_name = 'ScheduledPayments'; pk_col = 'id'\n",
    "\n",
    "if 'cleaned_dataframes' not in locals() or cleaned_dataframes.get(logical_name) is None or cleaned_dataframes.get(logical_name).empty:\n",
    "    print(f\"ERROR/SKIP: cleaned_dataframes['{logical_name}'] not found or empty.\")\n",
    "else:\n",
    "    df_cleaned = cleaned_dataframes[logical_name]\n",
    "    print(f\"Processing '{final_table_name}' with {len(df_cleaned)} rows.\")\n",
    "    DB_SERVER = 'WIN-A5I5TM5OKQQ\\\\SQLEXPRESS'; DB_DATABASE = 'BankingDB'; USE_TRUSTED_CONNECTION = True; driver = '{ODBC Driver 17 for SQL Server}'\n",
    "    cnxn = None; cursor = None\n",
    "    def get_sql_type(dtype):\n",
    "        if pd.api.types.is_integer_dtype(dtype): return 'BIGINT'\n",
    "        elif pd.api.types.is_float_dtype(dtype): return 'FLOAT'\n",
    "        elif pd.api.types.is_datetime64_any_dtype(dtype): return 'DATETIME2'\n",
    "        elif pd.api.types.is_bool_dtype(dtype): return 'BIT'\n",
    "        else: return 'NVARCHAR(MAX)'\n",
    "    def convert_value(item):\n",
    "        if pd.isna(item): return None\n",
    "        if isinstance(item, (np.integer)): return int(item)\n",
    "        if isinstance(item, (np.floating)): return float(item)\n",
    "        if isinstance(item, (np.bool_)): return bool(item)\n",
    "        return item\n",
    "    try:\n",
    "        conn_str = (f\"DRIVER={driver};SERVER={DB_SERVER};DATABASE={DB_DATABASE};Trusted_Connection=yes;\")\n",
    "        cnxn = pyodbc.connect(conn_str, autocommit=False)\n",
    "        cursor = cnxn.cursor()\n",
    "        print(\"  Connection established.\")\n",
    "        # CREATE TABLE\n",
    "        print(f\"  Creating table '{final_table_name}'...\")\n",
    "        column_defs, valid_sql_cols = [], []\n",
    "        for col_name in df_cleaned.columns:\n",
    "            if not isinstance(col_name, str) or not col_name: continue\n",
    "            sql_col_name=col_name; sql_type=get_sql_type(df_cleaned[col_name].dtype)\n",
    "            column_defs.append(f\"[{sql_col_name}] {sql_type}\"); valid_sql_cols.append(sql_col_name)\n",
    "        pk_def = f\", CONSTRAINT PK_{final_table_name} PRIMARY KEY ([{pk_col}])\" if pk_col in valid_sql_cols else \"\"\n",
    "        create_sql = f\"CREATE TABLE [{final_table_name}] ({', '.join(column_defs)} {pk_def} )\"\n",
    "        cursor.execute(f\"DROP TABLE IF EXISTS [{final_table_name}]\")\n",
    "        cursor.execute(create_sql); print(f\"  Table '{final_table_name}' created.\")\n",
    "        # INSERT INTO\n",
    "        print(f\"  Preparing and inserting rows...\")\n",
    "        sql_cols_str = ', '.join([f'[{c}]' for c in valid_sql_cols])\n",
    "        placeholders = ', '.join('?' * len(valid_sql_cols))\n",
    "        sql_insert = f\"INSERT INTO [{final_table_name}] ({sql_cols_str}) VALUES ({placeholders})\"\n",
    "        data_tuples = [tuple(convert_value(item) for item in row) for row in df_cleaned[valid_sql_cols].itertuples(index=False, name=None)]\n",
    "        if data_tuples:\n",
    "            cursor.fast_executemany = True; cursor.executemany(sql_insert, data_tuples)\n",
    "            print(f\"  SUCCESS: Inserted {cursor.rowcount} rows.\")\n",
    "        else: print(\"    Warning: No data to insert.\")\n",
    "        cnxn.commit(); print(f\"  Changes for '{final_table_name}' committed.\")\n",
    "    except Exception as e: print(f\"  ERROR processing table '{final_table_name}': {e}\"); cnxn.rollback()\n",
    "    finally:\n",
    "        if cursor: cursor.close()\n",
    "        if cnxn: cnxn.close()\n",
    "        print(\"  Connection closed.\")\n",
    "print(f\"--- Step 3G: Load Table '{final_table_name}' --- END ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3H (Final FK Fix): Add Foreign Keys --- START ---\n",
      "\n",
      "[FK Add] Setting up connection...\n",
      "[FK Add] Connection established.\n",
      "[FK Add] Attempting to add Foreign Keys...\n",
      "  Attempting to drop existing FKs first...\n",
      "  Finished attempting to drop existing FKs.\n",
      "  Attempting to CREATE Foreign Keys...\n",
      "    Adding FK: FK_Cards_Users (Cards.user_id -> Users.id)...\n",
      "      FK 'FK_Cards_Users' add command prepared.\n",
      "    Adding FK: FK_Transactions_Cards_From (Transactions.from_card_id -> Cards.id)...\n",
      "      FK 'FK_Transactions_Cards_From' add command prepared.\n",
      "    Adding FK: FK_Transactions_Cards_To (Transactions.to_card_id -> Cards.id)...\n",
      "      FK 'FK_Transactions_Cards_To' add command prepared.\n",
      "    Adding FK: FK_Logs_Transactions (Logs.transaction_id -> Transactions.id)...\n",
      "      FK 'FK_Logs_Transactions' add command prepared.\n",
      "    Adding FK: FK_ScheduledPayments_Cards (ScheduledPayments.card_id -> Cards.id)...\n",
      "      FK 'FK_ScheduledPayments_Cards' add command prepared.\n",
      "\n",
      "[FK Add] Committing FK changes...\n",
      "[FK Add] FK changes committed successfully.\n",
      "\n",
      "[FK Add] Closing connection...\n",
      "[FK Add] Connection closed.\n",
      "\n",
      "--- Step 3H (Final FK Fix): Add Foreign Keys --- END ---\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import urllib\n",
    "\n",
    "print(\"\\n--- Step 3H (Final FK Fix): Add Foreign Keys --- START ---\")\n",
    "\n",
    "DB_SERVER = 'WIN-A5I5TM5OKQQ\\\\SQLEXPRESS'; DB_DATABASE = 'BankingDB'; USE_TRUSTED_CONNECTION = True; driver = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "scheduled_payments_card_column = 'card_id' \n",
    "\n",
    "foreign_keys_to_create = [\n",
    "    ('Cards', 'FK_Cards_Users', 'user_id', 'Users', 'id'),\n",
    "    ('Transactions', 'FK_Transactions_Cards_From', 'from_card_id', 'Cards', 'id'),\n",
    "    ('Transactions', 'FK_Transactions_Cards_To', 'to_card_id', 'Cards', 'id'),\n",
    "    ('Logs', 'FK_Logs_Transactions', 'transaction_id', 'Transactions', 'id'),\n",
    "    # ScheduledPayments uchun child_column endi yuqoridagi o'zgaruvchidan olinadi\n",
    "    ('ScheduledPayments', 'FK_ScheduledPayments_Cards', scheduled_payments_card_column, 'Cards', 'id')\n",
    "]\n",
    "\n",
    "# --- Tekshirish: Placeholder o'zgartirilganmi? ---\n",
    "if 'HAQIQIY_USTUN_NOMINI_BU_YERGA_YOZING' in scheduled_payments_card_column:\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(\"XATO: 'scheduled_payments_card_column' o'zgaruvchisiga haqiqiy ustun nomini kiritmadingiz!\")\n",
    "    print(\"Iltimos, SSMSdan 'dbo.ScheduledPayments' jadvalidagi kartaga ishora qiluvchi ustun nomini toping va koddagi 'HAQIQIY_USTUN_NOMINI_BU_YERGA_YOZING' o'rniga yozing.\")\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    exit() # Skriptni to'xtatish\n",
    "\n",
    "cnxn = None; cursor = None\n",
    "print(\"\\n[FK Add] Setting up connection...\")\n",
    "try:\n",
    "    conn_str = (f\"DRIVER={driver};SERVER={DB_SERVER};DATABASE={DB_DATABASE};Trusted_Connection=yes;\")\n",
    "    cnxn = pyodbc.connect(conn_str, autocommit=False) # Autocommit FALSE\n",
    "    cursor = cnxn.cursor()\n",
    "    print(\"[FK Add] Connection established.\")\n",
    "\n",
    "    # --- FKlarni Qo'shish ---\n",
    "    print(\"[FK Add] Attempting to add Foreign Keys...\")\n",
    "    all_fks_added = True\n",
    "    # Avval o'chirish\n",
    "    print(\"  Attempting to drop existing FKs first...\")\n",
    "    for child_table, constraint_name, _, _, _ in reversed(foreign_keys_to_create):\n",
    "        try:\n",
    "            cursor.execute(f\"IF OBJECT_ID(?, 'F') IS NOT NULL ALTER TABLE [{child_table}] DROP CONSTRAINT [{constraint_name}]\", (constraint_name,))\n",
    "        except Exception: pass\n",
    "    print(\"  Finished attempting to drop existing FKs.\")\n",
    "    cnxn.commit()\n",
    "\n",
    "    # Endi qo'shish\n",
    "    print(\"  Attempting to CREATE Foreign Keys...\")\n",
    "    for child_table, constraint_name, child_column, parent_table, parent_column in foreign_keys_to_create:\n",
    "        print(f\"    Adding FK: {constraint_name} ({child_table}.{child_column} -> {parent_table}.{parent_column})...\")\n",
    "        try:\n",
    "            sql_fk = f\"\"\"\n",
    "                ALTER TABLE [{child_table}] ADD CONSTRAINT [{constraint_name}]\n",
    "                FOREIGN KEY ([{child_column}]) REFERENCES [{parent_table}]([{parent_column}])\n",
    "            \"\"\"\n",
    "            cursor.execute(sql_fk)\n",
    "            print(f\"      FK '{constraint_name}' add command prepared.\")\n",
    "        except pyodbc.Error as e:\n",
    "            print(f\"      ERROR preparing/adding Foreign Key '{constraint_name}': {e}\")\n",
    "            all_fks_added = False\n",
    "        except Exception as e:\n",
    "            print(f\"      Unexpected ERROR adding FK '{constraint_name}': {e}\")\n",
    "            all_fks_added = False\n",
    "\n",
    "    # --- Yakuniy Commit yoki Rollback ---\n",
    "    if all_fks_added:\n",
    "        print(\"\\n[FK Add] Committing FK changes...\")\n",
    "        cnxn.commit()\n",
    "        print(\"[FK Add] FK changes committed successfully.\")\n",
    "    else:\n",
    "        print(\"\\n[FK Add] Errors occurred during FK creation. Rolling back...\")\n",
    "        cnxn.rollback()\n",
    "        print(\"[FK Add] Rollback complete due to FK errors.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FK Add] A MAJOR error occurred: {e}\")\n",
    "    if cnxn: cnxn.rollback()\n",
    "finally:\n",
    "    print(\"\\n[FK Add] Closing connection...\")\n",
    "    if cursor: cursor.close()\n",
    "    if cnxn: cnxn.close()\n",
    "    print(\"[FK Add] Connection closed.\")\n",
    "\n",
    "print(\"\\n--- Step 3H (Final FK Fix): Add Foreign Keys --- END ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
